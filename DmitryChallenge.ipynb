{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/noobylub/Computational-Linguistic/blob/master/DmitryChallenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Optional, Union\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import random"
      ],
      "metadata": {
        "id": "SvMLwcM0fuAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiArmedBandit:\n",
        "    \"\"\"\n",
        "    Multi-armed bandit with Gaussian payout distributions.\n",
        "\n",
        "    Each arm has an independent Gaussian distribution with specified mean and variance.\n",
        "    The bandit tracks the history of arm pulls and can provide statistics for learning algorithms.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_arms: int = 10,\n",
        "        means: Optional[Union[List[float], np.ndarray, torch.Tensor]] = None,\n",
        "        variances: Optional[Union[List[float], np.ndarray, torch.Tensor]] = None,\n",
        "        random_seed: Optional[int] = None,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the multi-armed bandit.\n",
        "\n",
        "        Args:\n",
        "            n_arms: Number of arms in the bandit (default: 10)\n",
        "            means: Mean payout for each arm. If None, defaults to 0 for all arms.\n",
        "                  Can be list, numpy array, or torch tensor.\n",
        "            variances: Variance of payout for each arm. If None, defaults to 1 for all arms.\n",
        "                      Can be list, numpy array, or torch tensor.\n",
        "            random_seed: Random seed for reproducible results (default: None)\n",
        "        \"\"\"\n",
        "        self.n_arms = n_arms\n",
        "\n",
        "        # Set random seed if provided\n",
        "        if random_seed is not None:\n",
        "            torch.manual_seed(random_seed)\n",
        "            np.random.seed(random_seed)\n",
        "\n",
        "        # Initialize means\n",
        "        if means is None:\n",
        "            self.means = torch.zeros(n_arms)\n",
        "        else:\n",
        "            self.means = torch.tensor(means, dtype=torch.float32)\n",
        "            if len(self.means) != n_arms:\n",
        "                raise ValueError(\n",
        "                    f\"Length of means ({len(self.means)}) must equal n_arms ({n_arms})\"\n",
        "                )\n",
        "\n",
        "        # Initialize variances\n",
        "        if variances is None:\n",
        "            self.variances = torch.ones(n_arms)\n",
        "        else:\n",
        "            self.variances = torch.tensor(variances, dtype=torch.float32)\n",
        "            if len(self.variances) != n_arms:\n",
        "                raise ValueError(\n",
        "                    f\"Length of variances ({len(self.variances)}) must equal n_arms ({n_arms})\"\n",
        "                )\n",
        "            if torch.any(self.variances <= 0):\n",
        "                raise ValueError(\"All variances must be positive\")\n",
        "\n",
        "        # Compute standard deviations for efficiency\n",
        "        self.std_devs = torch.sqrt(self.variances)\n",
        "\n",
        "        # Initialize tracking variables\n",
        "        self.reset_history()\n",
        "\n",
        "    def reset_history(self):\n",
        "        \"\"\"Reset all tracking variables to initial state.\"\"\"\n",
        "        self.pull_counts = torch.zeros(self.n_arms, dtype=torch.long)\n",
        "        self.total_rewards = torch.zeros(self.n_arms)\n",
        "        self.sum_squared_rewards = torch.zeros(self.n_arms)\n",
        "        self.pull_history = []  # List of (arm, reward) tuples\n",
        "        self.total_pulls = 0\n",
        "\n",
        "    def pull_arm(self, arm: int) -> float:\n",
        "        \"\"\"\n",
        "        Pull a specific arm and get a reward sample.\n",
        "\n",
        "        Args:\n",
        "            arm: Index of the arm to pull (0 to n_arms-1)\n",
        "\n",
        "        Returns:\n",
        "            Reward sampled from the arm's Gaussian distribution\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If arm index is invalid\n",
        "        \"\"\"\n",
        "        if not (0 <= arm < self.n_arms):\n",
        "            raise ValueError(f\"Arm index {arm} is out of range [0, {self.n_arms - 1}]\")\n",
        "\n",
        "        # Sample reward from Gaussian distribution\n",
        "        reward = torch.normal(self.means[arm], self.std_devs[arm]).item()\n",
        "\n",
        "        # Update tracking variables\n",
        "        self.pull_counts[arm] += 1\n",
        "        self.total_rewards[arm] += reward\n",
        "        self.sum_squared_rewards[arm] += reward**2\n",
        "        self.pull_history.append((arm, reward))\n",
        "        self.total_pulls += 1\n",
        "\n",
        "        return reward\n",
        "\n",
        "    def get_arm_statistics(self, arm: int) -> dict:\n",
        "        \"\"\"\n",
        "        Get statistics for a specific arm.\n",
        "\n",
        "        Args:\n",
        "            arm: Index of the arm\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing statistics for the arm\n",
        "        \"\"\"\n",
        "        if not (0 <= arm < self.n_arms):\n",
        "            raise ValueError(f\"Arm index {arm} is out of range [0, {self.n_arms - 1}]\")\n",
        "\n",
        "        count = self.pull_counts[arm].item()\n",
        "\n",
        "        if count == 0:\n",
        "            return {\n",
        "                \"count\": 0,\n",
        "                \"mean_reward\": 0.0,\n",
        "                \"std_reward\": 0.0,\n",
        "                \"true_mean\": self.means[arm].item(),\n",
        "                \"true_std\": self.std_devs[arm].item(),\n",
        "            }\n",
        "\n",
        "        total_reward = self.total_rewards[arm].item()\n",
        "        sum_squared = self.sum_squared_rewards[arm].item()\n",
        "\n",
        "        mean_reward = total_reward / count\n",
        "\n",
        "        # Calculate sample standard deviation\n",
        "        if count > 1:\n",
        "            variance_sample = (sum_squared - count * mean_reward**2) / (count - 1)\n",
        "            std_reward = np.sqrt(max(0, variance_sample))  # Ensure non-negative\n",
        "        else:\n",
        "            std_reward = 0.0\n",
        "\n",
        "        return {\n",
        "            \"count\": count,\n",
        "            \"mean_reward\": mean_reward,\n",
        "            \"std_reward\": std_reward,\n",
        "            \"true_mean\": self.means[arm].item(),\n",
        "            \"true_std\": self.std_devs[arm].item(),\n",
        "        }\n",
        "\n",
        "    def get_all_statistics(self) -> dict:\n",
        "        \"\"\"\n",
        "        Get statistics for all arms.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with overall statistics and per-arm statistics\n",
        "        \"\"\"\n",
        "        arm_stats = [self.get_arm_statistics(i) for i in range(self.n_arms)]\n",
        "\n",
        "        return {\n",
        "            \"total_pulls\": self.total_pulls,\n",
        "            \"arms\": arm_stats,\n",
        "            \"overall_mean_reward\": (\n",
        "                self.total_rewards.sum().item() / max(1, self.total_pulls)\n",
        "            ),\n",
        "        }\n",
        "\n",
        "    def get_current_state(self) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Get current state suitable for neural network input.\n",
        "\n",
        "        Returns state in the format expected by MultiArmedBanditNet:\n",
        "        [usage_count_0, avg_payout_0, std_0, usage_count_1, avg_payout_1, std_1, ...]\n",
        "\n",
        "        Returns:\n",
        "            Tensor of shape (3 * n_arms,) containing the current state\n",
        "        \"\"\"\n",
        "        state = torch.zeros(3 * self.n_arms)\n",
        "\n",
        "        for arm in range(self.n_arms):\n",
        "            stats = self.get_arm_statistics(arm)\n",
        "            state[3 * arm] = stats[\"count\"]  # Usage count\n",
        "            state[3 * arm + 1] = stats[\"mean_reward\"]  # Average payout\n",
        "            state[3 * arm + 2] = stats[\"std_reward\"]  # Standard deviation\n",
        "\n",
        "        return state\n",
        "\n",
        "    def get_optimal_arm(self) -> int:\n",
        "        \"\"\"\n",
        "        Get the arm with the highest true mean (optimal arm for this bandit).\n",
        "\n",
        "        Returns:\n",
        "            Index of the optimal arm\n",
        "        \"\"\"\n",
        "        return torch.argmax(self.means).item()\n",
        "\n",
        "    def get_regret(self, arm: int) -> float:\n",
        "        \"\"\"\n",
        "        Calculate the instantaneous regret for pulling a specific arm.\n",
        "\n",
        "        Args:\n",
        "            arm: Index of the pulled arm\n",
        "\n",
        "        Returns:\n",
        "            Regret (difference between optimal and chosen arm's true mean)\n",
        "        \"\"\"\n",
        "        optimal_mean = self.means[self.get_optimal_arm()]\n",
        "        chosen_mean = self.means[arm]\n",
        "        return (optimal_mean - chosen_mean).item()\n",
        "\n",
        "    def get_cumulative_regret(self) -> float:\n",
        "        \"\"\"\n",
        "        Calculate cumulative regret over all pulls.\n",
        "\n",
        "        Returns:\n",
        "            Total regret accumulated over all arm pulls\n",
        "        \"\"\"\n",
        "        optimal_mean = self.means[self.get_optimal_arm()]\n",
        "        cumulative_regret = 0.0\n",
        "\n",
        "        for arm, _ in self.pull_history:\n",
        "            cumulative_regret += self.get_regret(arm)\n",
        "\n",
        "        return cumulative_regret\n",
        "\n",
        "    def simulate_batch_pulls(self, arms: List[int]) -> List[float]:\n",
        "        \"\"\"\n",
        "        Pull multiple arms in sequence and return all rewards.\n",
        "\n",
        "        Args:\n",
        "            arms: List of arm indices to pull\n",
        "\n",
        "        Returns:\n",
        "            List of rewards from each pull\n",
        "        \"\"\"\n",
        "        rewards = []\n",
        "        for arm in arms:\n",
        "            reward = self.pull_arm(arm)\n",
        "            rewards.append(reward)\n",
        "        return rewards\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        \"\"\"String representation of the bandit.\"\"\"\n",
        "        return (\n",
        "            f\"MultiArmedBandit(n_arms={self.n_arms}, \"\n",
        "            f\"total_pulls={self.total_pulls})\\n\"\n",
        "            f\"True means: {self.means.tolist()}\\n\"\n",
        "            f\"True stds: {self.std_devs.tolist()}\"\n",
        "        )\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        \"\"\"Detailed representation of the bandit.\"\"\"\n",
        "        return self.__str__()"
      ],
      "metadata": {
        "id": "pw-8qzIFgJQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiArmedBanditNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Neural network for multi-armed bandit action selection with parallel arm processing.\n",
        "\n",
        "    Input: 3 * n_arms features (for each arm: usage count, average payout, standard deviation)\n",
        "    Output: n_arms + 1 logits (one for each arm + stopping action)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_arms,\n",
        "        max_actions,\n",
        "        dim_arm=64,\n",
        "        hidden_layers=None,\n",
        "        dropout_rate=0.1,\n",
        "        activation=\"relu\",\n",
        "    ):\n",
        "        super(MultiArmedBanditNet, self).__init__()\n",
        "\n",
        "        self.n_arms = n_arms\n",
        "        self.max_actions = max_actions\n",
        "        self.dim_arm = dim_arm\n",
        "\n",
        "        if hidden_layers is None:\n",
        "            hidden_layers = [64, 32]\n",
        "\n",
        "        arm_input_dim = 3\n",
        "\n",
        "        if activation == \"relu\":\n",
        "            act_module = nn.ReLU()\n",
        "        elif activation == \"tanh\":\n",
        "            act_module = nn.Tanh()\n",
        "        elif activation == \"leaky_relu\":\n",
        "            act_module = nn.LeakyReLU(negative_slope=0.01)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported activation function: {}\".format(activation))\n",
        "\n",
        "        arm_embed_layers = [\n",
        "            nn.Linear(arm_input_dim, dim_arm),\n",
        "            act_module,\n",
        "            nn.BatchNorm1d(dim_arm),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(dim_arm, dim_arm),\n",
        "            act_module,\n",
        "            nn.BatchNorm1d(dim_arm),\n",
        "            nn.Dropout(dropout_rate),\n",
        "        ]\n",
        "        self.arm_embedder = nn.Sequential(*arm_embed_layers)\n",
        "\n",
        "        main_layers = []\n",
        "        prev_dim = dim_arm\n",
        "        for hidden_dim in hidden_layers:\n",
        "            main_layers.extend(\n",
        "                [\n",
        "                    nn.Linear(prev_dim, hidden_dim),\n",
        "                    act_module,\n",
        "                    nn.BatchNorm1d(hidden_dim),\n",
        "                    nn.Dropout(dropout_rate),\n",
        "                ]\n",
        "            )\n",
        "            prev_dim = hidden_dim\n",
        "        main_layers.append(nn.Linear(prev_dim, 1))\n",
        "        self.arm_logit_mlp = nn.Sequential(*main_layers)\n",
        "\n",
        "        context_dim = 3\n",
        "        stop_layers = [\n",
        "            nn.Linear(context_dim, 32),\n",
        "            act_module,\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(32, 16),\n",
        "            act_module,\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(16, 1),\n",
        "        ]\n",
        "        self.stop_logit_mlp = nn.Sequential(*stop_layers)\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for module in self.modules():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                nn.init.xavier_uniform_(module.weight)\n",
        "                if module.bias is not None:\n",
        "                    nn.init.constant_(module.bias, 0.0)\n",
        "\n",
        "    def _embed_arms(self, arm_features):\n",
        "        # arm_features: (batch_size, n_arms, 3)\n",
        "        batch_size, n_arms, feature_dim = arm_features.shape\n",
        "        flattened_arms = arm_features.reshape(-1, feature_dim)\n",
        "        arm_embeddings = self.arm_embedder(flattened_arms)\n",
        "        arm_embeddings = arm_embeddings.reshape(batch_size, n_arms, self.dim_arm)\n",
        "        return arm_embeddings\n",
        "\n",
        "    def _compute_arm_logits(self, arm_embeddings):\n",
        "        # arm_embeddings: (batch_size, n_arms, dim_arm)\n",
        "        batch_size, n_arms, dim_arm = arm_embeddings.shape\n",
        "        flattened_embeddings = arm_embeddings.reshape(-1, dim_arm)\n",
        "        arm_logits_flat = self.arm_logit_mlp(flattened_embeddings)\n",
        "        arm_logits = arm_logits_flat.reshape(batch_size, n_arms)\n",
        "        return arm_logits\n",
        "\n",
        "    def _compute_stop_logit(self, arm_features):\n",
        "        # arm_features: (batch_size, n_arms, 3)\n",
        "        # context: total_actions (normalized), weighted_avg_reward, arms_used_ratio\n",
        "        usage_counts = arm_features[:, :, 0]\n",
        "        avg_payouts = arm_features[:, :, 1]\n",
        "\n",
        "        total_actions = torch.sum(usage_counts, dim=1, keepdim=True) / max(\n",
        "            1.0, float(self.max_actions)\n",
        "        )\n",
        "        total_usage = torch.sum(usage_counts, dim=1, keepdim=True)\n",
        "        total_usage = torch.clamp(total_usage, min=1e-8)\n",
        "        weighted_avg_reward = (\n",
        "            torch.sum(usage_counts * avg_payouts, dim=1, keepdim=True) / total_usage\n",
        "        )\n",
        "        arms_used = (usage_counts > 0).float()\n",
        "        arms_used_ratio = torch.sum(arms_used, dim=1, keepdim=True) / float(self.n_arms)\n",
        "\n",
        "        context = torch.cat(\n",
        "            [total_actions, weighted_avg_reward, arms_used_ratio], dim=1\n",
        "        )\n",
        "        stop_logit = self.stop_logit_mlp(context)\n",
        "        return stop_logit\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, 3 * n_arms)\n",
        "        expected_input_dim = 3 * self.n_arms\n",
        "        if x.shape[-1] != expected_input_dim:\n",
        "            raise ValueError(\n",
        "                \"Expected input dimension {}, got {}\".format(\n",
        "                    expected_input_dim, x.shape[-1]\n",
        "                )\n",
        "            )\n",
        "\n",
        "        batch_size = x.shape[0]\n",
        "        arm_features = x.reshape(batch_size, self.n_arms, 3)\n",
        "\n",
        "        normalized_arm_features = arm_features.clone()\n",
        "        normalized_arm_features[:, :, 0] = normalized_arm_features[:, :, 0] / float(\n",
        "            self.max_actions\n",
        "        )\n",
        "\n",
        "        arm_embeddings = self._embed_arms(normalized_arm_features)\n",
        "        arm_logits = self._compute_arm_logits(arm_embeddings)\n",
        "        stop_logit = self._compute_stop_logit(arm_features)\n",
        "\n",
        "        all_logits = torch.cat([arm_logits, stop_logit], dim=1)\n",
        "        return all_logits\n",
        "\n",
        "    def get_action_probabilities(self, x, temperature=1.0):\n",
        "        logits = self.forward(x)\n",
        "        probabilities = F.softmax(logits / float(temperature), dim=-1)\n",
        "        return probabilities\n",
        "\n",
        "    def select_action(self, x, temperature=1.0, deterministic=False):\n",
        "        with torch.no_grad():\n",
        "            probabilities = self.get_action_probabilities(x, temperature)\n",
        "            if deterministic:\n",
        "                actions = torch.argmax(probabilities, dim=-1)\n",
        "            else:\n",
        "                # multinomial expects probabilities to sum to 1 across last dimension\n",
        "                actions = torch.multinomial(probabilities, num_samples=1).squeeze(-1)\n",
        "            return actions\n",
        "\n",
        "    def get_arm_embeddings(self, x):\n",
        "        batch_size = x.shape[0]\n",
        "        arm_features = x.reshape(batch_size, self.n_arms, 3)\n",
        "        normalized_arm_features = arm_features.clone()\n",
        "        normalized_arm_features[:, :, 0] = normalized_arm_features[:, :, 0] / float(\n",
        "            self.max_actions\n",
        "        )\n",
        "        return self._embed_arms(normalized_arm_features)\n",
        "\n",
        "    def get_arm_logits_separate(self, x):\n",
        "        batch_size = x.shape[0]\n",
        "        arm_features = x.reshape(batch_size, self.n_arms, 3)\n",
        "        normalized_arm_features = arm_features.clone()\n",
        "        normalized_arm_features[:, :, 0] = normalized_arm_features[:, :, 0] / float(\n",
        "            self.max_actions\n",
        "        )\n",
        "        arm_embeddings = self._embed_arms(normalized_arm_features)\n",
        "        arm_logits = self._compute_arm_logits(arm_embeddings)\n",
        "        stop_logit = self._compute_stop_logit(arm_features)\n",
        "        return arm_logits, stop_logit\n",
        "\n",
        "    # def train_q_learning(\n",
        "    #     self,\n",
        "    #     bandit,\n",
        "    #     optimizer,\n",
        "    #     num_steps=1000,\n",
        "    #     epsilon=0.1,\n",
        "    #     epsilon_decay=0.995,\n",
        "    #     min_epsilon=0.01,\n",
        "    #     buffer_size=10000,\n",
        "    #     batch_size=32,\n",
        "    #     target_update_freq=100,\n",
        "    #     gamma=0.99,\n",
        "    # ):\n",
        "    #     # Experience replay buffer\n",
        "    #     replay_buffer = deque(maxlen=buffer_size)\n",
        "\n",
        "    #     # Target network (not strictly necessary for stateless bandits)\n",
        "    #     target_net = MultiArmedBanditNet(self.n_arms, self.max_actions, self.dim_arm)\n",
        "    #     target_net.load_state_dict(self.state_dict())\n",
        "    #     target_net.eval()\n",
        "\n",
        "    #     metrics = {\n",
        "    #         \"losses\": [],\n",
        "    #         \"rewards\": [],\n",
        "    #         \"cumulative_regret\": [],\n",
        "    #         \"epsilon_values\": [],\n",
        "    #         \"q_values\": [],\n",
        "    #     }\n",
        "\n",
        "    #     current_epsilon = epsilon\n",
        "    #     total_reward = 0.0\n",
        "\n",
        "    #     for step in range(num_steps):\n",
        "    #         state = bandit.get_current_state().unsqueeze(0)\n",
        "\n",
        "    #         # Training logic\n",
        "    #         decision_value = random.random()\n",
        "    #         if decision_value > current_epsilon:\n",
        "    #             # Exploit\n",
        "    #             with torch.no_grad():\n",
        "    #                 predicted_q_values = policy_network(state)\n",
        "    #             arm_index = np.argmax(predicted_q_values)\n",
        "    #             # Not allowing the player to stop\n",
        "    #         else:\n",
        "    #             # Take a random action\n",
        "    #             arm_index = random.randomint(0, bandit.n_arms - 1)\n",
        "    #         reward = bandit.pull_arm(arm_index)\n",
        "    #         next_state = bandit.get_current_state()\n",
        "    #         replay_buffer.append((state, next_state, reward))\n",
        "\n",
        "    #         if step % 100 == 0:\n",
        "    #             with torch.no_grad():\n",
        "    #                 q_vals = self.forward(state)\n",
        "    #                 metrics[\"q_values\"].append(q_vals.mean().item())\n",
        "    #             metrics[\"rewards\"].append(total_reward)\n",
        "    #             metrics[\"cumulative_regret\"].append(bandit.get_cumulative_regret())\n",
        "    #             metrics[\"epsilon_values\"].append(current_epsilon)\n",
        "\n",
        "    #         current_epsilon *= epsilon_decay\n",
        "\n",
        "    #     # Use the buffer for the actual training\n",
        "    #     batch = random.choices(replay_buffer, batch_size)\n",
        "    #     states, next_states, rewards = zip(*replay_buffer)\n",
        "    #     start_state_tensor = torch.tensor(states)\n",
        "    #     # We need to extract one Q-value\n",
        "    #     q_value_tensor = policy_network(start_state_tensor)\n",
        "    #     # Now use the arm_index to look at the Q-value of the action\n",
        "    #     # we actually took\n",
        "    #     next_q_values = target_network(next_states)\n",
        "    #     # Do the same here\n",
        "\n",
        "    #     reference_q_values = r # + next_q_values\n",
        "    #     # length = batch_size, one q-value for each action taken\n",
        "    #     loss = np.square(q_value_tensor - reference_q_values).mean()\n",
        "\n",
        "\n",
        "\n",
        "    #     # Debug output every 200 steps (moved outside the if step % 100 block)\n",
        "    #     if step % 200 == 0:\n",
        "    #         current_loss = metrics[\"losses\"][-1] if metrics[\"losses\"] else 0.0\n",
        "\n",
        "    #         with torch.no_grad():\n",
        "    #             q_vals = self.forward(state)\n",
        "    #             q_vals_np = q_vals.cpu().numpy().flatten()\n",
        "\n",
        "    #         print(\n",
        "    #             f\"Step {step}: Loss={current_loss:.4f}, Reward={total_reward:.3f}, \"\n",
        "    #             f\"Regret={bandit.get_cumulative_regret():.3f}, \"\n",
        "    #             f\"Epsilon={current_epsilon:.3f}\"\n",
        "    #         )\n",
        "    #         print(f\"  Q-values: [{', '.join([f'{q:.3f}' for q in q_vals_np])}]\")\n",
        "\n",
        "    #         # Show arm statistics\n",
        "    #         stats = bandit.get_all_statistics()\n",
        "    #         arm_counts = [arm[\"count\"] for arm in stats[\"arms\"]]\n",
        "    #         arm_means = [arm[\"mean_reward\"] for arm in stats[\"arms\"]]\n",
        "    #         print(f\"  Arm pulls: {arm_counts}\")\n",
        "    #         print(f\"  Arm means: [{', '.join([f'{r:.3f}' for r in arm_means])}]\")\n",
        "\n",
        "    #         # Show best action\n",
        "    #         best_action = torch.argmax(q_vals).item()\n",
        "    #         if best_action < self.n_arms:\n",
        "    #             print(f\"  Best action: Arm {best_action}\")\n",
        "    #         else:\n",
        "    #             print(f\"  Best action: Stop\")\n",
        "    #         print()\n",
        "\n",
        "    #     # Final debug output\n",
        "    #     print(f\"\\nFinal Training Summary:\")\n",
        "    #     with torch.no_grad():\n",
        "    #         final_state = bandit.get_current_state().unsqueeze(0)\n",
        "    #         final_q_vals = self.forward(final_state)\n",
        "    #         final_q_vals_np = final_q_vals.cpu().numpy().flatten()\n",
        "    #         best_final_action = torch.argmax(final_q_vals).item()\n",
        "\n",
        "    #     print(\n",
        "    #         f\"Final Q-values (at end of training): [{', '.join([f'{q:.3f}' for q in final_q_vals_np])}]\"\n",
        "    #     )\n",
        "    #     if best_final_action < self.n_arms:\n",
        "    #         print(f\"Final preferred action: Arm {best_final_action}\")\n",
        "    #     else:\n",
        "    #         print(f\"Final preferred action: Stop\")\n",
        "\n",
        "    #     stats = bandit.get_all_statistics()\n",
        "    #     arm_counts = [arm[\"count\"] for arm in stats[\"arms\"]]\n",
        "    #     arm_means = [arm[\"mean_reward\"] for arm in stats[\"arms\"]]\n",
        "    #     print(f\"Final arm pulls: {arm_counts}\")\n",
        "    #     print(f\"Final arm means: [{', '.join([f'{r:.3f}' for r in arm_means])}]\")\n",
        "    #     print(f\"Total training steps: {num_steps}\")\n",
        "    #     print()\n",
        "\n",
        "    #     # Store final values for consistent reporting\n",
        "    #     metrics[\"final_q_values\"] = final_q_vals_np\n",
        "    #     metrics[\"final_state\"] = final_state\n",
        "    #     metrics[\"final_best_action\"] = best_final_action\n",
        "\n",
        "    #     return metrics\n",
        "\n",
        "\n",
        "    def train_q_learning(\n",
        "        self,\n",
        "        bandit,\n",
        "        optimizer,\n",
        "        num_steps=1000,\n",
        "        epsilon=0.1,\n",
        "        epsilon_decay=0.995,\n",
        "        min_epsilon=0.01,\n",
        "        buffer_size=10000,\n",
        "        batch_size=32,\n",
        "        target_update_freq=100,\n",
        "        gamma=0.99,\n",
        "    ):\n",
        "        # Experience replay buffer\n",
        "        replay_buffer = deque(maxlen=buffer_size)\n",
        "\n",
        "        # Target network (not strictly necessary for stateless bandits but kept for structure)\n",
        "        target_net = MultiArmedBanditNet(self.n_arms, self.max_actions, self.dim_arm)\n",
        "        target_net.load_state_dict(self.state_dict())\n",
        "        target_net.eval()\n",
        "\n",
        "        metrics = {\n",
        "            \"losses\": [],\n",
        "            \"rewards\": [],\n",
        "            \"cumulative_regret\": [],\n",
        "            \"epsilon_values\": [],\n",
        "            \"q_values\": [],\n",
        "        }\n",
        "\n",
        "        current_epsilon = epsilon\n",
        "        total_reward = 0.0\n",
        "\n",
        "        for step in range(num_steps):\n",
        "            state = bandit.get_current_state().unsqueeze(0)\n",
        "\n",
        "            if random.random() < current_epsilon:\n",
        "                action = random.randint(0, self.n_arms)  # includes stop action\n",
        "            else:\n",
        "                with torch.no_grad():\n",
        "                    q_values = self.forward(state)\n",
        "                    action = torch.argmax(q_values, dim=1).item()\n",
        "\n",
        "            if action < self.n_arms:\n",
        "                reward = bandit.pull_arm(action)\n",
        "            else:\n",
        "                reward = -0.05  # Stronger penalty for stopping\n",
        "\n",
        "            total_reward += reward\n",
        "            next_state = bandit.get_current_state().unsqueeze(0)\n",
        "\n",
        "            replay_buffer.append(\n",
        "                (state.squeeze(0), action, reward, next_state.squeeze(0))\n",
        "            )\n",
        "\n",
        "            if len(replay_buffer) >= batch_size:\n",
        "                # Look at a small subset of the actions and rewards\n",
        "                # to approximate independent and identically distributed\n",
        "                # random sampling of state visits\n",
        "                batch = random.sample(replay_buffer, batch_size)\n",
        "                states, actions, rewards, next_states = zip(*batch)\n",
        "\n",
        "                states = torch.stack(states)\n",
        "                actions = torch.tensor(actions, dtype=torch.long)\n",
        "                rewards = torch.tensor(rewards, dtype=torch.float32)\n",
        "                next_states = torch.stack(next_states)\n",
        "\n",
        "                current_q_values = self.forward(states)\n",
        "                # Extract Q-values for the actions that we actually\n",
        "                # took in one go using fast indexing\n",
        "                current_q_values = current_q_values.gather(\n",
        "                    1, actions.unsqueeze(1)\n",
        "                ).squeeze(1)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    next_q_values = target_net.forward(next_states)\n",
        "                    max_next_q_values = torch.max(next_q_values, dim=1)[0]\n",
        "                    target_q_values = rewards + gamma * max_next_q_values\n",
        "\n",
        "                # MSE = mean squared error\n",
        "                loss = F.mse_loss(current_q_values, target_q_values)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                metrics[\"losses\"].append(loss.item())\n",
        "\n",
        "            if step % target_update_freq == 0:\n",
        "                target_net.load_state_dict(self.state_dict())\n",
        "\n",
        "            current_epsilon = max(min_epsilon, current_epsilon * epsilon_decay)\n",
        "\n",
        "            if step % 100 == 0:\n",
        "                with torch.no_grad():\n",
        "                    q_vals = self.forward(state)\n",
        "                    metrics[\"q_values\"].append(q_vals.mean().item())\n",
        "                metrics[\"rewards\"].append(total_reward)\n",
        "                metrics[\"cumulative_regret\"].append(bandit.get_cumulative_regret())\n",
        "                metrics[\"epsilon_values\"].append(current_epsilon)\n",
        "\n",
        "        # Debug output every 200 steps (moved outside the if step % 100 block)\n",
        "        if step % 200 == 0:\n",
        "            current_loss = metrics[\"losses\"][-1] if metrics[\"losses\"] else 0.0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                q_vals = self.forward(state)\n",
        "                q_vals_np = q_vals.cpu().numpy().flatten()\n",
        "\n",
        "            print(\n",
        "                f\"Step {step}: Loss={current_loss:.4f}, Reward={total_reward:.3f}, \"\n",
        "                f\"Regret={bandit.get_cumulative_regret():.3f}, \"\n",
        "                f\"Epsilon={current_epsilon:.3f}\"\n",
        "            )\n",
        "            print(f\"  Q-values: [{', '.join([f'{q:.3f}' for q in q_vals_np])}]\")\n",
        "\n",
        "            # Show arm statistics\n",
        "            stats = bandit.get_all_statistics()\n",
        "            arm_counts = [arm[\"count\"] for arm in stats[\"arms\"]]\n",
        "            arm_means = [arm[\"mean_reward\"] for arm in stats[\"arms\"]]\n",
        "            print(f\"  Arm pulls: {arm_counts}\")\n",
        "            print(f\"  Arm means: [{', '.join([f'{r:.3f}' for r in arm_means])}]\")\n",
        "\n",
        "            # Show best action\n",
        "            best_action = torch.argmax(q_vals).item()\n",
        "            if best_action < self.n_arms:\n",
        "                print(f\"  Best action: Arm {best_action}\")\n",
        "            else:\n",
        "                print(f\"  Best action: Stop\")\n",
        "            print()\n",
        "\n",
        "        # Final debug output - store state and Q-values for consistency\n",
        "        print(f\"\\nFinal Training Summary:\")\n",
        "        with torch.no_grad():\n",
        "            final_state = bandit.get_current_state().unsqueeze(0)\n",
        "            final_q_vals = self.forward(final_state)\n",
        "            final_q_vals_np = final_q_vals.cpu().numpy().flatten()\n",
        "            best_final_action = torch.argmax(final_q_vals).item()\n",
        "\n",
        "        print(\n",
        "            f\"Final Q-values (at end of training): [{', '.join([f'{q:.3f}' for q in final_q_vals_np])}]\"\n",
        "        )\n",
        "        if best_final_action < self.n_arms:\n",
        "            print(f\"Final preferred action: Arm {best_final_action}\")\n",
        "        else:\n",
        "            print(f\"Final preferred action: Stop\")\n",
        "\n",
        "        stats = bandit.get_all_statistics()\n",
        "        arm_counts = [arm[\"count\"] for arm in stats[\"arms\"]]\n",
        "        arm_means = [arm[\"mean_reward\"] for arm in stats[\"arms\"]]\n",
        "        print(f\"Final arm pulls: {arm_counts}\")\n",
        "        print(f\"Final arm means: [{', '.join([f'{r:.3f}' for r in arm_means])}]\")\n",
        "        print(f\"Total training steps: {num_steps}\")\n",
        "        print()\n",
        "\n",
        "        # Store final values for consistent reporting\n",
        "        metrics[\"final_q_values\"] = final_q_vals_np\n",
        "        metrics[\"final_state\"] = final_state\n",
        "        metrics[\"final_best_action\"] = best_final_action\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def train_reinforce(\n",
        "        self,\n",
        "        bandit,\n",
        "        optimizer,\n",
        "        num_episodes=500,\n",
        "        max_steps_per_episode=20,\n",
        "        temperature=1.0,\n",
        "        baseline_lr=0.01,\n",
        "    ):\n",
        "        # Baseline network for variance reduction\n",
        "        baseline_net = torch.nn.Sequential(\n",
        "            torch.nn.Linear(3 * self.n_arms, 32),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(32, 1),\n",
        "        )\n",
        "        baseline_optimizer = optim.Adam(baseline_net.parameters(), lr=baseline_lr)\n",
        "\n",
        "        metrics = {\n",
        "            \"episode_returns\": [],\n",
        "            \"episode_lengths\": [],\n",
        "            \"policy_losses\": [],\n",
        "            \"baseline_losses\": [],\n",
        "            \"cumulative_regret\": [],\n",
        "        }\n",
        "\n",
        "        arm_selection_counts = [0] * self.n_arms\n",
        "        stop_count = 0\n",
        "\n",
        "        for episode in range(num_episodes):\n",
        "            states = []\n",
        "            actions = []\n",
        "            rewards = []\n",
        "            log_probs = []\n",
        "            episode_arm_selections = [0] * self.n_arms\n",
        "\n",
        "            for step in range(max_steps_per_episode):\n",
        "                # Complete the episode\n",
        "\n",
        "            if len(rewards) == 0:\n",
        "                # no steps taken\n",
        "                continue\n",
        "\n",
        "            # Compute discounted returns with gamma=0.99\n",
        "            returns = torch.zeros(len(rewards), dtype=torch.float32)\n",
        "            g_acc = 0.0\n",
        "            gamma = 0.99\n",
        "            for t in range(len(rewards) - 1, -1, -1):\n",
        "                # TODO\n",
        "\n",
        "            # Stack log_probs and states\n",
        "            log_probs = torch.stack(log_probs).squeeze(-1)\n",
        "            states_tensor = torch.stack(states).squeeze(1)\n",
        "\n",
        "            if log_probs.dim() == 0:\n",
        "                log_probs = log_probs.unsqueeze(0)\n",
        "            if states_tensor.dim() == 1:\n",
        "                states_tensor = states_tensor.unsqueeze(0)\n",
        "\n",
        "            # Enhanced baseline using state features\n",
        "            baseline_values = baseline_net(states_tensor).squeeze()\n",
        "            if baseline_values.dim() == 0:\n",
        "                baseline_values = baseline_values.unsqueeze(0)\n",
        "\n",
        "            advantages = returns - baseline_values.detach()\n",
        "\n",
        "            # Normalize advantages for stability\n",
        "            if len(advantages) > 1:\n",
        "                advantages = (advantages - advantages.mean()) / (\n",
        "                    advantages.std() + 1e-8\n",
        "                )\n",
        "\n",
        "            # Policy loss with importance weighting\n",
        "            # TODO\n",
        "            policy_loss = -log_probs * advantages\n",
        "\n",
        "            # Baseline loss\n",
        "            # TODO\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            policy_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            baseline_optimizer.zero_grad()\n",
        "            baseline_loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(baseline_net.parameters(), max_norm=0.5)\n",
        "            baseline_optimizer.step()\n",
        "\n",
        "            episode_return = float(returns[0].item())\n",
        "            metrics[\"episode_returns\"].append(episode_return)\n",
        "            metrics[\"episode_lengths\"].append(len(rewards))\n",
        "            metrics[\"policy_losses\"].append(float(policy_loss.item()))\n",
        "            metrics[\"baseline_losses\"].append(float(baseline_loss.item()))\n",
        "            metrics[\"cumulative_regret\"].append(bandit.get_cumulative_regret())\n",
        "\n",
        "            # Enhanced progress reporting\n",
        "            if episode % 50 == 0:\n",
        "                last = (\n",
        "                    metrics[\"episode_returns\"][-50:]\n",
        "                    if len(metrics[\"episode_returns\"]) >= 50\n",
        "                    else metrics[\"episode_returns\"]\n",
        "                )\n",
        "                avg_return = np.mean(last) if len(last) > 0 else 0.0\n",
        "                avg_length = (\n",
        "                    np.mean(metrics[\"episode_lengths\"][-50:])\n",
        "                    if len(metrics[\"episode_lengths\"]) >= 1\n",
        "                    else 0.0\n",
        "                )\n",
        "\n",
        "                # Calculate recent arm preferences\n",
        "                recent_episodes = min(50, episode + 1)\n",
        "                recent_selections = arm_selection_counts.copy()\n",
        "                recent_total = sum(recent_selections) + stop_count\n",
        "\n",
        "                print(\n",
        "                    f\"Episode {episode:3d}: Return={avg_return:6.3f}, Length={avg_length:4.1f}, \"\n",
        "                    f\"Loss={metrics['policy_losses'][-1]:6.4f}, Regret={bandit.get_cumulative_regret():6.1f}\"\n",
        "                )\n",
        "\n",
        "                if recent_total > 0:\n",
        "                    arm_prefs = [\n",
        "                        f\"{100 * count / recent_total:4.1f}%\"\n",
        "                        for count in recent_selections\n",
        "                    ]\n",
        "                    stop_pref = f\"{100 * stop_count / recent_total:4.1f}%\"\n",
        "                    print(f\"         Arms: [{', '.join(arm_prefs)}], Stop: {stop_pref}\")\n",
        "\n",
        "        # Final analysis\n",
        "        print(f\"\\nðŸ“Š REINFORCE Training Summary:\")\n",
        "        print(f\"   Total episodes: {num_episodes}\")\n",
        "        total_selections = sum(arm_selection_counts) + stop_count\n",
        "        if total_selections > 0:\n",
        "            print(f\"   Action distribution:\")\n",
        "            for i, count in enumerate(arm_selection_counts):\n",
        "                pct = 100 * count / total_selections\n",
        "                print(f\"     Arm {i}: {count:4d} selections ({pct:5.1f}%)\")\n",
        "            stop_pct = 100 * stop_count / total_selections\n",
        "            print(f\"     Stop:   {stop_count:4d} selections ({stop_pct:5.1f}%)\")\n",
        "        print()\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def evaluate_policy(\n",
        "        self, bandit, num_episodes=100, max_steps_per_episode=20, deterministic=True\n",
        "    ):\n",
        "        self.eval()\n",
        "        total_rewards = []\n",
        "        episode_lengths = []\n",
        "        regrets = []\n",
        "\n",
        "        # Save current bandit state\n",
        "        original_pull_counts = bandit.pull_counts.clone()\n",
        "        original_total_rewards = bandit.total_rewards.clone()\n",
        "        original_sum_squared_rewards = bandit.sum_squared_rewards.clone()\n",
        "        original_pull_history = bandit.pull_history.copy()\n",
        "        original_total_pulls = bandit.total_pulls\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for episode in range(num_episodes):\n",
        "                # Reset to original state for each episode\n",
        "                bandit.pull_counts = original_pull_counts.clone()\n",
        "                bandit.total_rewards = original_total_rewards.clone()\n",
        "                bandit.sum_squared_rewards = original_sum_squared_rewards.clone()\n",
        "                bandit.pull_history = original_pull_history.copy()\n",
        "                bandit.total_pulls = original_total_pulls\n",
        "\n",
        "                episode_reward = 0.0\n",
        "                steps_taken = 0\n",
        "                initial_regret = bandit.get_cumulative_regret()\n",
        "\n",
        "                for step in range(max_steps_per_episode):\n",
        "                    state = bandit.get_current_state().unsqueeze(0)\n",
        "                    action = self.select_action(\n",
        "                        state, deterministic=deterministic\n",
        "                    ).item()\n",
        "                    if action < self.n_arms:\n",
        "                        reward = bandit.pull_arm(action)\n",
        "                        episode_reward += reward\n",
        "                        steps_taken += 1\n",
        "                    else:\n",
        "                        break\n",
        "\n",
        "                total_rewards.append(episode_reward)\n",
        "                episode_lengths.append(steps_taken)\n",
        "                regrets.append(bandit.get_cumulative_regret() - initial_regret)\n",
        "\n",
        "        # Restore original bandit state\n",
        "        bandit.pull_counts = original_pull_counts\n",
        "        bandit.total_rewards = original_total_rewards\n",
        "        bandit.sum_squared_rewards = original_sum_squared_rewards\n",
        "        bandit.pull_history = original_pull_history\n",
        "        bandit.total_pulls = original_total_pulls\n",
        "\n",
        "        self.train()\n",
        "\n",
        "        return {\n",
        "            \"mean_reward\": float(np.mean(total_rewards)),\n",
        "            \"std_reward\": float(np.std(total_rewards)),\n",
        "            \"mean_length\": float(np.mean(episode_lengths)),\n",
        "            \"std_length\": float(np.std(episode_lengths)),\n",
        "            \"mean_regret\": float(np.mean(regrets)),\n",
        "            \"std_regret\": float(np.std(regrets)),\n",
        "        }\n",
        "\n",
        "    def evaluate_policy_simple(self, bandit, num_actions=100, deterministic=True):\n",
        "        \"\"\"\n",
        "        Simple evaluation that doesn't reset bandit history.\n",
        "        Evaluates the learned policy by taking actions and measuring performance.\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        initial_regret = bandit.get_cumulative_regret()\n",
        "        initial_total_reward = sum(reward for _, reward in bandit.pull_history)\n",
        "\n",
        "        actions_taken = 0\n",
        "        episode_reward = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for _ in range(num_actions):\n",
        "                state = bandit.get_current_state().unsqueeze(0)\n",
        "\n",
        "                if deterministic:\n",
        "                    q_values = self.forward(state)\n",
        "                    action = torch.argmax(q_values).item()\n",
        "                else:\n",
        "                    action = self.select_action(\n",
        "                        state, deterministic=deterministic\n",
        "                    ).item()\n",
        "\n",
        "                if action < self.n_arms:\n",
        "                    reward = bandit.pull_arm(action)\n",
        "                    episode_reward += reward\n",
        "                    actions_taken += 1\n",
        "                else:\n",
        "                    # Don't break on stop action during evaluation - force exploration\n",
        "                    forced_action = torch.randint(0, self.n_arms, (1,)).item()\n",
        "                    reward = bandit.pull_arm(forced_action)\n",
        "                    episode_reward += reward\n",
        "                    actions_taken += 1\n",
        "\n",
        "        final_regret = bandit.get_cumulative_regret()\n",
        "        evaluation_regret = final_regret - initial_regret\n",
        "\n",
        "        self.train()\n",
        "\n",
        "        return {\n",
        "            \"total_reward\": episode_reward,\n",
        "            \"mean_reward\": episode_reward / max(1, actions_taken),\n",
        "            \"actions_taken\": actions_taken,\n",
        "            \"evaluation_regret\": evaluation_regret,\n",
        "        }"
      ],
      "metadata": {
        "id": "8lT2Ae-jfxXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_bandit_state(\n",
        "    n_arms, usage_counts, average_payouts, std_deviations, device=\"cpu\"\n",
        "):\n",
        "    if (\n",
        "        len(usage_counts) != n_arms\n",
        "        or len(average_payouts) != n_arms\n",
        "        or len(std_deviations) != n_arms\n",
        "    ):\n",
        "        raise ValueError(\n",
        "            \"Usage counts, average payouts, and standard deviations must have length n_arms\"\n",
        "        )\n",
        "\n",
        "    state = torch.zeros(1, 3 * n_arms, device=device)\n",
        "    for i in range(n_arms):\n",
        "        state[0, 3 * i] = usage_counts[i]\n",
        "        state[0, 3 * i + 1] = average_payouts[i]\n",
        "        state[0, 3 * i + 2] = std_deviations[i]\n",
        "    return state"
      ],
      "metadata": {
        "id": "_IiE-Gegf5K1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate_q_learning(\n",
        "    n_arms=5,\n",
        "    bandit_means=None,\n",
        "    bandit_variances=None,\n",
        "    num_steps=2000,\n",
        "    learning_rate=0.01,\n",
        "):\n",
        "\n",
        "    if bandit_means is None:\n",
        "        bandit_means = [0.1, 0.5, 0.3, 0.8, 0.2]\n",
        "    if bandit_variances is None:\n",
        "        bandit_variances = [0.1] * n_arms\n",
        "\n",
        "    bandit = MultiArmedBandit(\n",
        "        n_arms=n_arms, means=bandit_means, variances=bandit_variances, random_seed=42\n",
        "    )\n",
        "\n",
        "    net = MultiArmedBanditNet(n_arms, max_actions=50)\n",
        "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
        "\n",
        "    print(\"â•”\" + \"â•\" * 58 + \"â•—\")\n",
        "    print(\"â•‘\" + \" Q-LEARNING TRAINING \".center(58) + \"â•‘\")\n",
        "    print(\"â•š\" + \"â•\" * 58 + \"â•\")\n",
        "    print(f\"ðŸŽ¯ Problem Setup:\")\n",
        "    print(f\"   Arms: {n_arms}\")\n",
        "    print(f\"   True means: {bandit_means}\")\n",
        "    print(\n",
        "        f\"   Optimal arm: {bandit.get_optimal_arm()} (mean = {max(bandit_means):.3f})\"\n",
        "    )\n",
        "    print(f\"   Training steps: {num_steps}\")\n",
        "    print()\n",
        "\n",
        "    # Store initial state for comparison\n",
        "    initial_regret = bandit.get_cumulative_regret()\n",
        "\n",
        "    metrics = net.train_q_learning(\n",
        "        bandit,\n",
        "        optimizer,\n",
        "        num_steps=num_steps,\n",
        "        epsilon=0.5,\n",
        "        min_epsilon=0.1,\n",
        "        epsilon_decay=0.995,\n",
        "    )\n",
        "\n",
        "    # Use simple evaluation that preserves learned bandit state\n",
        "    eval_metrics_simple = net.evaluate_policy_simple(bandit, num_actions=100)\n",
        "\n",
        "    # Also try traditional evaluation for comparison\n",
        "    eval_metrics = net.evaluate_policy(bandit, num_episodes=100)\n",
        "\n",
        "    if metrics.get(\"losses\"):\n",
        "        print(\"Final loss: {:.4f}\".format(metrics[\"losses\"][-1]))\n",
        "    if metrics.get(\"cumulative_regret\"):\n",
        "        training_regret = metrics[\"cumulative_regret\"][-1] - initial_regret\n",
        "        print(\"Training regret: {:.3f}\".format(training_regret))\n",
        "        print(\n",
        "            \"Final cumulative regret: {:.3f}\".format(metrics[\"cumulative_regret\"][-1])\n",
        "        )\n",
        "\n",
        "    # Show both evaluation results\n",
        "    print(\"\\n--- Simple Evaluation (preserves bandit state) ---\")\n",
        "    print(\"Mean reward: {:.3f}\".format(eval_metrics_simple[\"mean_reward\"]))\n",
        "    print(\"Actions taken: {}\".format(eval_metrics_simple[\"actions_taken\"]))\n",
        "    print(\"Evaluation regret: {:.3f}\".format(eval_metrics_simple[\"evaluation_regret\"]))\n",
        "\n",
        "    print(\"\\n--- Traditional Evaluation (resets each episode) ---\")\n",
        "    print(\n",
        "        \"Mean reward: {:.3f} Â± {:.3f}\".format(\n",
        "            eval_metrics[\"mean_reward\"], eval_metrics[\"std_reward\"]\n",
        "        )\n",
        "    )\n",
        "    print(\"Mean regret: {:.3f}\".format(eval_metrics[\"mean_regret\"]))\n",
        "\n",
        "    # Comprehensive learning analysis\n",
        "    print(\"â•”\" + \"â•\" * 58 + \"â•—\")\n",
        "    print(\"â•‘\" + \" Q-LEARNING RESULTS ANALYSIS \".center(58) + \"â•‘\")\n",
        "    print(\"â•š\" + \"â•\" * 58 + \"â•\")\n",
        "\n",
        "    # Use stored final Q-values for consistency\n",
        "    if \"final_q_values\" in metrics:\n",
        "        final_q_vals_np = metrics[\"final_q_values\"]\n",
        "        best_action = metrics[\"final_best_action\"]\n",
        "    else:\n",
        "        # Fallback if not stored (shouldn't happen)\n",
        "        with torch.no_grad():\n",
        "            final_state = bandit.get_current_state().unsqueeze(0)\n",
        "            final_q_vals = net.forward(final_state)\n",
        "            final_q_vals_np = final_q_vals.cpu().numpy().flatten()\n",
        "            best_action = torch.argmax(final_q_vals).item()\n",
        "\n",
        "    print(f\"ðŸ§  Learned Q-values (consistent with training end):\")\n",
        "    for i in range(len(bandit_means)):\n",
        "        print(\n",
        "            f\"   Arm {i}: Q={final_q_vals_np[i]:7.3f} (true mean={bandit_means[i]:.3f})\"\n",
        "        )\n",
        "    print(f\"   Stop:  Q={final_q_vals_np[-1]:7.3f}\")\n",
        "    print()\n",
        "\n",
        "    print(f\"ðŸŽ¯ Decision Analysis:\")\n",
        "    optimal_arm = bandit.get_optimal_arm()\n",
        "    if best_action < len(bandit_means):\n",
        "        print(\n",
        "            f\"   Agent prefers: Arm {best_action} (mean = {bandit_means[best_action]:.3f})\"\n",
        "        )\n",
        "        if best_action == optimal_arm:\n",
        "            print(\"   âœ… SUCCESS: Learned to prefer optimal arm!\")\n",
        "        else:\n",
        "            print(f\"   âš ï¸  SUBOPTIMAL: Optimal is arm {optimal_arm}\")\n",
        "    else:\n",
        "        print(\"   âŒ PROBLEM: Agent prefers to stop\")\n",
        "\n",
        "    # Performance assessment\n",
        "    optimal_reward = max(bandit_means)\n",
        "    performance_ratio = eval_metrics_simple[\"mean_reward\"] / optimal_reward\n",
        "    print(f\"\\nðŸ“ˆ Performance Metrics:\")\n",
        "    print(f\"   Achieved: {eval_metrics_simple['mean_reward']:.3f}\")\n",
        "    print(f\"   Optimal:  {optimal_reward:.3f}\")\n",
        "    print(f\"   Ratio:    {performance_ratio:.1%}\")\n",
        "\n",
        "    if performance_ratio >= 0.95:\n",
        "        print(\"   ðŸ† EXCELLENT: >95% of optimal!\")\n",
        "    elif performance_ratio >= 0.8:\n",
        "        print(\"   âœ… GOOD: >80% of optimal\")\n",
        "    elif performance_ratio >= 0.6:\n",
        "        print(\"   âš ï¸  FAIR: >60% of optimal\")\n",
        "    else:\n",
        "        print(\"   âŒ POOR: <60% of optimal\")\n",
        "\n",
        "    print()\n",
        "\n",
        "    return net, metrics, eval_metrics"
      ],
      "metadata": {
        "id": "JChTmErmf_L6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uE0O7WwlfaDZ"
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate_reinforce(\n",
        "    n_arms=5,\n",
        "    bandit_means=None,\n",
        "    bandit_variances=None,\n",
        "    num_episodes=1000,\n",
        "    learning_rate=0.01,\n",
        "):\n",
        "    if bandit_means is None:\n",
        "        bandit_means = [0.1, 0.5, 0.3, 0.8, 0.2]\n",
        "    if bandit_variances is None:\n",
        "        bandit_variances = [0.1] * n_arms\n",
        "\n",
        "    bandit = MultiArmedBandit(\n",
        "        n_arms=n_arms, means=bandit_means, variances=bandit_variances, random_seed=42\n",
        "    )\n",
        "\n",
        "    net = MultiArmedBanditNet(n_arms, max_actions=50)\n",
        "    optimizer = optim.Adam(\n",
        "        net.parameters(), lr=learning_rate * 10\n",
        "    )  # Higher learning rate for REINFORCE\n",
        "\n",
        "    print(\"â•”\" + \"â•\" * 58 + \"â•—\")\n",
        "    print(\"â•‘\" + \" REINFORCE TRAINING \".center(58) + \"â•‘\")\n",
        "    print(\"â•š\" + \"â•\" * 58 + \"â•\")\n",
        "    print(f\"ðŸŽ¯ Problem Setup:\")\n",
        "    print(f\"   Arms: {n_arms}\")\n",
        "    print(f\"   True means: {bandit_means}\")\n",
        "    print(\n",
        "        f\"   Optimal arm: {bandit.get_optimal_arm()} (mean = {max(bandit_means):.3f})\"\n",
        "    )\n",
        "    print(f\"   Episodes: {num_episodes}\")\n",
        "    print()\n",
        "\n",
        "    # Pre-populate bandit with some initial experience to help learning\n",
        "    print(\"ðŸ”„ Pre-populating bandit with initial experience...\")\n",
        "    for arm in range(n_arms):\n",
        "        for _ in range(3):  # Give each arm 3 initial pulls\n",
        "            bandit.pull_arm(arm)\n",
        "\n",
        "    metrics = net.train_reinforce(\n",
        "        bandit, optimizer, num_episodes=num_episodes, temperature=1.5, baseline_lr=0.001\n",
        "    )\n",
        "    eval_metrics = net.evaluate_policy(bandit, num_episodes=100)\n",
        "\n",
        "    print(\"â•”\" + \"â•\" * 58 + \"â•—\")\n",
        "    print(\"â•‘\" + \" REINFORCE RESULTS ANALYSIS \".center(58) + \"â•‘\")\n",
        "    print(\"â•š\" + \"â•\" * 58 + \"â•\")\n",
        "\n",
        "    print(f\"ðŸ“Š Training Metrics:\")\n",
        "    if metrics.get(\"policy_losses\"):\n",
        "        print(f\"   Final policy loss: {metrics['policy_losses'][-1]:.4f}\")\n",
        "    if metrics.get(\"cumulative_regret\"):\n",
        "        print(f\"   Final cumulative regret: {metrics['cumulative_regret'][-1]:.3f}\")\n",
        "\n",
        "    # Learning curve analysis\n",
        "    if len(metrics[\"episode_returns\"]) >= 100:\n",
        "        early_returns = np.mean(metrics[\"episode_returns\"][:50])\n",
        "        late_returns = np.mean(metrics[\"episode_returns\"][-50:])\n",
        "        improvement = late_returns - early_returns\n",
        "        print(f\"   Early episodes (0-49): {early_returns:.3f} avg return\")\n",
        "        print(f\"   Late episodes (-50:-1): {late_returns:.3f} avg return\")\n",
        "        print(f\"   Improvement: {improvement:+.3f}\")\n",
        "\n",
        "    print(f\"\\nðŸ“ˆ Evaluation Results:\")\n",
        "    print(\n",
        "        f\"   Mean reward: {eval_metrics['mean_reward']:.3f} Â± {eval_metrics['std_reward']:.3f}\"\n",
        "    )\n",
        "    print(\n",
        "        f\"   Mean episode length: {eval_metrics['mean_length']:.1f} Â± {eval_metrics['std_length']:.1f}\"\n",
        "    )\n",
        "    print(f\"   Mean regret: {eval_metrics['mean_regret']:.3f}\")\n",
        "\n",
        "    # Performance assessment\n",
        "    optimal_reward = max(bandit_means)\n",
        "    performance_ratio = (\n",
        "        eval_metrics[\"mean_reward\"] / (eval_metrics[\"mean_length\"] * optimal_reward)\n",
        "        if eval_metrics[\"mean_length\"] > 0\n",
        "        else 0\n",
        "    )\n",
        "    print(f\"\\nðŸŽ¯ Performance Analysis:\")\n",
        "    print(\n",
        "        f\"   Per-step reward: {eval_metrics['mean_reward'] / max(1, eval_metrics['mean_length']):.3f}\"\n",
        "    )\n",
        "    print(f\"   Optimal per-step: {optimal_reward:.3f}\")\n",
        "    print(f\"   Efficiency: {performance_ratio:.1%}\")\n",
        "\n",
        "    if performance_ratio >= 0.8:\n",
        "        print(\"   ðŸ† EXCELLENT: >80% efficient!\")\n",
        "    elif performance_ratio >= 0.6:\n",
        "        print(\"   âœ… GOOD: >60% efficient\")\n",
        "    elif performance_ratio >= 0.4:\n",
        "        print(\"   âš ï¸  FAIR: >40% efficient\")\n",
        "    else:\n",
        "        print(\"   âŒ POOR: <40% efficient\")\n",
        "\n",
        "    print()\n",
        "\n",
        "    return net, metrics, eval_metrics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"â•”\" + \"â•\" * 78 + \"â•—\")\n",
        "print(\"â•‘\" + \" MULTI-ARMED BANDIT NEURAL NETWORK TRAINING DEMO \".center(78) + \"â•‘\")\n",
        "print(\"â•š\" + \"â•\" * 78 + \"â•\")\n",
        "print()\n",
        "\n",
        "n_arms = 5\n",
        "max_actions = 100\n",
        "\n",
        "bandit_net = MultiArmedBanditNet(\n",
        "    n_arms, max_actions, dim_arm=64, hidden_layers=[64, 32], dropout_rate=0.1\n",
        ")\n",
        "\n",
        "print(\"ðŸ”§ Network Architecture:\")\n",
        "print(f\"   Model: {type(bandit_net).__name__}\")\n",
        "print(f\"   Arms: {n_arms}\")\n",
        "print(f\"   Max actions: {max_actions}\")\n",
        "print(f\"   Parameters: {sum(p.numel() for p in bandit_net.parameters()):,}\")\n",
        "print()\n",
        "\n",
        "# Test with empty state\n",
        "state = create_bandit_state(n_arms, [0] * n_arms, [0.0] * n_arms, [0.0] * n_arms)\n",
        "logits = bandit_net(state)\n",
        "probabilities = bandit_net.get_action_probabilities(state)\n",
        "\n",
        "print(\"ðŸ§ª Network Testing (empty state):\")\n",
        "print(f\"   Output shape: {logits.shape}\")\n",
        "print(\n",
        "    f\"   Action probabilities: uniform = {probabilities.squeeze().detach().numpy()}\"\n",
        ")\n",
        "print()\n",
        "\n",
        "print(\"â”Œ\" + \"â”€\" * 76 + \"â”\")\n",
        "print(\"â”‚\" + \" TRAINING DEMONSTRATIONS \".center(76) + \"â”‚\")\n",
        "print(\"â””\" + \"â”€\" * 76 + \"â”˜\")\n",
        "print()\n",
        "\n",
        "# Q-learning demonstration\n",
        "try:\n",
        "    print(\"ðŸŽ¯ Starting Q-Learning demonstration...\")\n",
        "    q_net, q_metrics, q_eval = train_and_evaluate_q_learning(\n",
        "        n_arms=3, bandit_means=[0.3, 0.7, 0.1], num_steps=1000\n",
        "    )\n",
        "    print(\"âœ… Q-learning demonstration completed successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Q-learning demo failed: {e}\")\n",
        "    import traceback\n",
        "\n",
        "    traceback.print_exc()\n",
        "\n",
        "print(\"\\n\" + \"â”€\" * 80 + \"\\n\")\n",
        "\n",
        "# REINFORCE demonstration\n",
        "try:\n",
        "    print(\"ðŸŽ¯ Starting REINFORCE demonstration...\")\n",
        "    r_net, r_metrics, r_eval = train_and_evaluate_reinforce(\n",
        "        n_arms=3,\n",
        "        bandit_means=[0.3, 0.7, 0.1],\n",
        "        num_episodes=400,\n",
        "        learning_rate=0.005,\n",
        "    )\n",
        "    print(\"âœ… REINFORCE demonstration completed successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ REINFORCE demo failed: {e}\")\n",
        "    import traceback\n",
        "\n",
        "    traceback.print_exc()\n",
        "\n",
        "print(\"\\n\" + \"â”€\" * 80 + \"\\n\")\n",
        "\n",
        "# Educational comparison: Untrained vs Trained models\n",
        "print(\"ðŸ§ª Learning Effect Demonstration:\")\n",
        "\n",
        "try:\n",
        "    # Create a realistic test scenario\n",
        "    demo_bandit = MultiArmedBandit(\n",
        "        n_arms=3, means=[0.3, 0.7, 0.1], variances=[0.1, 0.1, 0.1], random_seed=456\n",
        "    )\n",
        "\n",
        "    # Simulate some exploration experience\n",
        "    print(\"   Creating realistic bandit state...\")\n",
        "    print(\"   - Pulling each arm several times to build statistics\")\n",
        "\n",
        "    for arm in range(3):\n",
        "        for _ in range(8):  # Each arm gets 8 pulls\n",
        "            demo_bandit.pull_arm(arm)\n",
        "\n",
        "    demo_state = demo_bandit.get_current_state().unsqueeze(0)\n",
        "\n",
        "    # Show the bandit statistics\n",
        "    stats = demo_bandit.get_all_statistics()\n",
        "    print(\"\\n   ðŸ“Š Bandit Statistics After Exploration:\")\n",
        "    for i, arm_stat in enumerate(stats[\"arms\"]):\n",
        "        print(\n",
        "            f\"     Arm {i}: {arm_stat['count']:2d} pulls, \"\n",
        "            f\"{arm_stat['mean_reward']:.3f} observed mean, \"\n",
        "            f\"{arm_stat['true_mean']:.3f} true mean\"\n",
        "        )\n",
        "\n",
        "    print(f\"   ðŸŽ¯ Optimal choice should be: Arm {demo_bandit.get_optimal_arm()}\")\n",
        "\n",
        "    # Compare untrained vs trained (if available)\n",
        "    print(\"\\n   ðŸ†š Model Comparison:\")\n",
        "\n",
        "    # Untrained network\n",
        "    untrained_net = MultiArmedBanditNet(3, max_actions=50)\n",
        "    with torch.no_grad():\n",
        "        untrained_prefs = untrained_net.get_action_probabilities(demo_state)\n",
        "        untrained_choice = torch.argmax(untrained_prefs).item()\n",
        "\n",
        "    print(f\"   ðŸ“ Untrained network:\")\n",
        "    print(\n",
        "        f\"     Preferences: {[f'{p:.3f}' for p in untrained_prefs.squeeze().detach().numpy()]}\"\n",
        "    )\n",
        "    print(\n",
        "        f\"     Choice: {'Arm ' + str(untrained_choice) if untrained_choice < 3 else 'Stop'}\"\n",
        "    )\n",
        "\n",
        "    # Trained networks (if available)\n",
        "    if \"q_net\" in locals():\n",
        "        q_net.eval()\n",
        "        with torch.no_grad():\n",
        "            q_prefs = q_net.get_action_probabilities(demo_state)\n",
        "            q_choice = torch.argmax(q_prefs).item()\n",
        "\n",
        "        print(f\"   ðŸŽ¯ Q-learning (trained):\")\n",
        "        print(\n",
        "            f\"     Preferences: {[f'{p:.3f}' for p in q_prefs.squeeze().detach().numpy()]}\"\n",
        "        )\n",
        "        print(f\"     Choice: {'Arm ' + str(q_choice) if q_choice < 3 else 'Stop'}\")\n",
        "\n",
        "        if q_choice == demo_bandit.get_optimal_arm():\n",
        "            print(\"     âœ… Correctly identifies optimal arm!\")\n",
        "        else:\n",
        "            print(\"     âŒ Suboptimal choice\")\n",
        "\n",
        "    if \"r_net\" in locals():\n",
        "        r_net.eval()\n",
        "        with torch.no_grad():\n",
        "            r_prefs = r_net.get_action_probabilities(demo_state)\n",
        "            r_choice = torch.argmax(r_prefs).item()\n",
        "\n",
        "        print(f\"   ðŸŽ¯ REINFORCE (trained):\")\n",
        "        print(\n",
        "            f\"     Preferences: {[f'{p:.3f}' for p in r_prefs.squeeze().detach().numpy()]}\"\n",
        "        )\n",
        "        print(f\"     Choice: {'Arm ' + str(r_choice) if r_choice < 3 else 'Stop'}\")\n",
        "\n",
        "        if r_choice == demo_bandit.get_optimal_arm():\n",
        "            print(\"     âœ… Correctly identifies optimal arm!\")\n",
        "        else:\n",
        "            print(\"     âŒ Suboptimal choice\")\n",
        "\n",
        "    if \"q_net\" not in locals() and \"r_net\" not in locals():\n",
        "        print(\"   (No trained models available for comparison)\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"   Demonstration failed: {e}\")\n",
        "    import traceback\n",
        "\n",
        "    traceback.print_exc()\n",
        "\n",
        "print(\"\\n\" + \"â•”\" + \"â•\" * 78 + \"â•—\")\n",
        "print(\"â•‘\" + \" DEMO COMPLETE \".center(78) + \"â•‘\")\n",
        "print(\"â•š\" + \"â•\" * 78 + \"â•\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1qn5oF3gbdV",
        "outputId": "d46230f3-7acb-4358-f086-4a5640f5e28c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
            "â•‘               MULTI-ARMED BANDIT NEURAL NETWORK TRAINING DEMO                â•‘\n",
            "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
            "\n",
            "ðŸ”§ Network Architecture:\n",
            "   Model: MultiArmedBanditNet\n",
            "   Arms: 5\n",
            "   Max actions: 100\n",
            "   Parameters: 11,810\n",
            "\n",
            "ðŸ§ª Network Testing (empty state):\n",
            "   Output shape: torch.Size([1, 6])\n",
            "   Action probabilities: uniform = [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
            "\n",
            "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
            "â”‚                          TRAINING DEMONSTRATIONS                           â”‚\n",
            "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
            "\n",
            "ðŸŽ¯ Starting Q-Learning demonstration...\n",
            "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
            "â•‘                   Q-LEARNING TRAINING                    â•‘\n",
            "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
            "ðŸŽ¯ Problem Setup:\n",
            "   Arms: 3\n",
            "   True means: [0.3, 0.7, 0.1]\n",
            "   Optimal arm: 1 (mean = 0.700)\n",
            "   Training steps: 1000\n",
            "\n",
            "\n",
            "Final Training Summary:\n",
            "Final Q-values (at end of training): [8.180, 7.636, 8.267, 8.345]\n",
            "Final preferred action: Stop\n",
            "Final arm pulls: [66, 382, 68]\n",
            "Final arm means: [0.305, 0.688, 0.069]\n",
            "Total training steps: 1000\n",
            "\n",
            "Final loss: 0.3919\n",
            "Training regret: 59.600\n",
            "Final cumulative regret: 59.600\n",
            "\n",
            "--- Simple Evaluation (preserves bandit state) ---\n",
            "Mean reward: 0.482\n",
            "Actions taken: 100\n",
            "Evaluation regret: 23.800\n",
            "\n",
            "--- Traditional Evaluation (resets each episode) ---\n",
            "Mean reward: 0.000 Â± 0.000\n",
            "Mean regret: 0.000\n",
            "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
            "â•‘               Q-LEARNING RESULTS ANALYSIS                â•‘\n",
            "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
            "ðŸ§  Learned Q-values (consistent with training end):\n",
            "   Arm 0: Q=  8.180 (true mean=0.300)\n",
            "   Arm 1: Q=  7.636 (true mean=0.700)\n",
            "   Arm 2: Q=  8.267 (true mean=0.100)\n",
            "   Stop:  Q=  8.345\n",
            "\n",
            "ðŸŽ¯ Decision Analysis:\n",
            "   âŒ PROBLEM: Agent prefers to stop\n",
            "\n",
            "ðŸ“ˆ Performance Metrics:\n",
            "   Achieved: 0.482\n",
            "   Optimal:  0.700\n",
            "   Ratio:    68.8%\n",
            "   âš ï¸  FAIR: >60% of optimal\n",
            "\n",
            "âœ… Q-learning demonstration completed successfully!\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "ðŸŽ¯ Starting REINFORCE demonstration...\n",
            "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
            "â•‘                    REINFORCE TRAINING                    â•‘\n",
            "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
            "ðŸŽ¯ Problem Setup:\n",
            "   Arms: 3\n",
            "   True means: [0.3, 0.7, 0.1]\n",
            "   Optimal arm: 1 (mean = 0.700)\n",
            "   Episodes: 400\n",
            "\n",
            "ðŸ”„ Pre-populating bandit with initial experience...\n",
            "Episode   0: Return= 1.346, Length= 3.0, Loss=-0.0851, Regret=   3.4\n",
            "         Arms: [33.3%, 33.3%,  0.0%], Stop: 33.3%\n",
            "Episode  50: Return= 8.821, Length=19.5, Loss=0.0017, Regret= 194.4\n",
            "         Arms: [39.8%, 53.8%,  6.0%], Stop:  0.3%\n",
            "Episode 100: Return=12.212, Length=20.0, Loss=-0.0000, Regret= 221.2\n",
            "         Arms: [23.1%, 73.8%,  3.0%], Stop:  0.2%\n",
            "Episode 150: Return=12.809, Length=20.0, Loss=-0.0000, Regret= 221.2\n",
            "         Arms: [15.3%, 82.6%,  2.0%], Stop:  0.1%\n",
            "Episode 200: Return=12.980, Length=20.0, Loss=0.0000, Regret= 221.2\n",
            "         Arms: [11.5%, 87.0%,  1.5%], Stop:  0.1%\n",
            "Episode 250: Return=12.715, Length=20.0, Loss=-0.0000, Regret= 221.2\n",
            "         Arms: [ 9.2%, 89.6%,  1.2%], Stop:  0.1%\n",
            "Episode 300: Return=12.499, Length=20.0, Loss=-0.0000, Regret= 221.2\n",
            "         Arms: [ 7.6%, 91.3%,  1.0%], Stop:  0.1%\n",
            "Episode 350: Return=12.386, Length=20.0, Loss=0.0000, Regret= 221.2\n",
            "         Arms: [ 6.5%, 92.6%,  0.8%], Stop:  0.0%\n",
            "\n",
            "ðŸ“Š REINFORCE Training Summary:\n",
            "   Total episodes: 400\n",
            "   Action distribution:\n",
            "     Arm 0:  457 selections (  5.7%)\n",
            "     Arm 1: 7440 selections ( 93.5%)\n",
            "     Arm 2:   59 selections (  0.7%)\n",
            "     Stop:      3 selections (  0.0%)\n",
            "\n",
            "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
            "â•‘                REINFORCE RESULTS ANALYSIS                â•‘\n",
            "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
            "ðŸ“Š Training Metrics:\n",
            "   Final policy loss: 0.0000\n",
            "   Final cumulative regret: 221.200\n",
            "   Early episodes (0-49): 8.716 avg return\n",
            "   Late episodes (-50:-1): 12.748 avg return\n",
            "   Improvement: +4.031\n",
            "\n",
            "ðŸ“ˆ Evaluation Results:\n",
            "   Mean reward: 14.106 Â± 1.599\n",
            "   Mean episode length: 20.0 Â± 0.0\n",
            "   Mean regret: 0.000\n",
            "\n",
            "ðŸŽ¯ Performance Analysis:\n",
            "   Per-step reward: 0.705\n",
            "   Optimal per-step: 0.700\n",
            "   Efficiency: 100.8%\n",
            "   ðŸ† EXCELLENT: >80% efficient!\n",
            "\n",
            "âœ… REINFORCE demonstration completed successfully!\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "ðŸ§ª Learning Effect Demonstration:\n",
            "   Creating realistic bandit state...\n",
            "   - Pulling each arm several times to build statistics\n",
            "\n",
            "   ðŸ“Š Bandit Statistics After Exploration:\n",
            "     Arm 0:  8 pulls, 0.295 observed mean, 0.300 true mean\n",
            "     Arm 1:  8 pulls, 0.782 observed mean, 0.700 true mean\n",
            "     Arm 2:  8 pulls, 0.068 observed mean, 0.100 true mean\n",
            "   ðŸŽ¯ Optimal choice should be: Arm 1\n",
            "\n",
            "   ðŸ†š Model Comparison:\n",
            "   ðŸ“ Untrained network:\n",
            "     Preferences: ['0.735', '0.046', '0.048', '0.171']\n",
            "     Choice: Arm 0\n",
            "   ðŸŽ¯ Q-learning (trained):\n",
            "     Preferences: ['0.257', '0.344', '0.225', '0.173']\n",
            "     Choice: Arm 1\n",
            "     âœ… Correctly identifies optimal arm!\n",
            "   ðŸŽ¯ REINFORCE (trained):\n",
            "     Preferences: ['0.000', '1.000', '0.000', '0.000']\n",
            "     Choice: Arm 1\n",
            "     âœ… Correctly identifies optimal arm!\n",
            "\n",
            "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
            "â•‘                                DEMO COMPLETE                                 â•‘\n",
            "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f93fC-G1glFP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}