{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/noobylub/Computational-Linguistic/blob/master/AttentionAsKernelRegression_(with_TODOs).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "v19piTUUQMKW",
        "outputId": "d3eae3db-c6b0-4e1e-c87f-dc17e3c564b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(442, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets, linear_model\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "# Load the diabetes dataset\n",
        "\n",
        "# Ten baseline variables, age, sex, body mass index, average blood pressure,\n",
        "# and six blood serum measurements were obtained for each of n = 442 diabetes\n",
        "# patients, as well as the response of interest, a quantitative measure of\n",
        "# disease progression one year after baseline.\n",
        "# Note: Each of these 10 feature variables have been mean centered and scaled by\n",
        "# the standard deviation times the square root of n_samples (i.e. the sum of\n",
        "# squares of each column totals 1).\n",
        "\n",
        "# DN note: scaling by 1/sqrt(n)*std instead of 1/std works better with\n",
        "# regularised regression.\n",
        "\n",
        "X, y = datasets.load_diabetes(return_X_y=True)\n",
        "X.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A good baseline: vanilla OLS linear regression"
      ],
      "metadata": {
        "id": "UjuvOaTvWeLU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "ols_y_pred = None\n",
        "\n",
        "# TODO: train a linear-regression model on X_train + y_train\n",
        "# and predict on X_test\n",
        "\n",
        "print(\"Mean squared error: %.2f\" % mean_squared_error(y_test, ols_y_pred))\n",
        "# The coefficient of determination: 1 is perfect prediction\n",
        "print(\"Coefficient of determination: %.2f\" % r2_score(y_test, ols_y_pred))"
      ],
      "metadata": {
        "id": "XILcWGE_Q-2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A dumb baseline: 1-nearest-neighbour regressor"
      ],
      "metadata": {
        "id": "6EzYKGMhWiFe"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ae56c6a1"
      },
      "source": [
        "def one_nearest_neighbor_regressor(X_train, y_train, X_test):\n",
        "    y_pred = []\n",
        "\n",
        "    # Calculate Euclidean distances from all elements of X_test\n",
        "    # to all training samples.\n",
        "    # Find the index of the closest training sample for each.\n",
        "    # Predict the value from y_train corresponding to the closest sample\n",
        "    # in X_train.\n",
        "\n",
        "    return y_pred\n",
        "\n",
        "ols_1nn_y_pred = one_nearest_neighbor_regressor(X_train, y_train, X_test)\n",
        "\n",
        "print(\"Mean squared error (1-NN): %.2f\" % mean_squared_error(y_test, ols_1nn_y_pred))\n",
        "print(\"Coefficient of determination (1-NN): %.2f\" % r2_score(y_test, ols_1nn_y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reducing variance: interpolate between k nearest neigbhours"
      ],
      "metadata": {
        "id": "2AB3Mlh2Wyb1"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87cd171b"
      },
      "source": [
        "def k_nearest_neighbor_regressor(X_train, y_train, X_test, k=3):\n",
        "    y_pred = []\n",
        "\n",
        "    # Calculate Euclidean distance to all training samples\n",
        "    # Get the indices of the k closest training samples\n",
        "    # Predict the average of y_train values corresponding to the k closest\n",
        "    # samples in X_train\n",
        "\n",
        "    return y_pred\n",
        "\n",
        "# To select k properly we need a dev set/cross-validation, but this will do\n",
        "# for comparison purposes\n",
        "best_knn_mse = float('inf')\n",
        "best_knn_r2 = 0.0\n",
        "best_k = None\n",
        "for k in [3, 5, 10, 15]:\n",
        "    knn_y_pred = k_nearest_neighbor_regressor(X_train, y_train, X_test, k=k)\n",
        "    k_mse = mean_squared_error(y_test, knn_y_pred)\n",
        "    print(f\"Mean squared error ({k}-NN): {k_mse:2f}\")\n",
        "    k_r2 = r2_score(y_test, knn_y_pred)\n",
        "    print(f\"Coefficient of determination ({k}-NN): {k_r2:2f}\")\n",
        "    if k_mse < best_knn_mse:\n",
        "        best_knn_mse = k_mse\n",
        "        best_knn_r2 = k_r2\n",
        "        best_k = k\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Weighted interpolation: weight all values in the training set by their distance to the test sample in the feature space, using dot products as distances"
      ],
      "metadata": {
        "id": "QDpUe56IW-ot"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "876ddb9a"
      },
      "source": [
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "\n",
        "def dot_product_softmax_regressor(X_train, y_train, X_test):\n",
        "    y_pred = []\n",
        "\n",
        "    # Similar to k_nearest_neighbor_regressor, but with dot product\n",
        "    # instead of Euclidean distances, passed through softmax to get weights,\n",
        "    # and with final result being\n",
        "    # a weighted average of all y_train elements.\n",
        "\n",
        "    return y_pred\n",
        "\n",
        "dot_softmax_y_pred = dot_product_softmax_regressor(X_train, y_train, X_test)\n",
        "\n",
        "print(\"Mean squared error (Dot Product Softmax Regressor): %.2f\" % mean_squared_error(y_test, dot_softmax_y_pred))\n",
        "print(\"Coefficient of determination (Dot Product Softmax Regressor): %.2f\" % r2_score(y_test, dot_softmax_y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Some preliminary work for training on the GPU"
      ],
      "metadata": {
        "id": "DEKrFPiCZEpi"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25dcc270"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Convert NumPy arrays to PyTorch tensors\n",
        "# Ensure X and X_test are float32, and y, y_train, y_test are float32 and\n",
        "# reshaped to (-1, 1)\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)\n",
        "\n",
        "# Split the original training data into new training and validation sets\n",
        "X_train_new, X_val, y_train_new, y_val = train_test_split(\n",
        "    X_train_tensor, y_train_tensor, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Move tensors to GPU\n",
        "X_train_new = X_train_new.to(device)\n",
        "y_train_new = y_train_new.to(device)\n",
        "X_val = X_val.to(device)\n",
        "y_val = y_val.to(device)\n",
        "X_test_tensor = X_test_tensor.to(device)\n",
        "y_test_tensor = y_test_tensor.to(device)\n",
        "X_train_tensor = X_train_tensor.to(device)\n",
        "y_train_tensor = y_train_tensor.to(device)\n",
        "\n",
        "train_dataset = TensorDataset(X_train_new, y_train_new)\n",
        "val_dataset = TensorDataset(X_val, y_val)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "# Create DataLoader objects to conveniently iterate over batches\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"New training set size: {len(train_dataset)} samples\")\n",
        "print(f\"Validation set size: {len(val_dataset)} samples\")\n",
        "print(f\"Test set size: {len(test_dataset)} samples\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's make the distance calculation more general by using a bilinear form"
      ],
      "metadata": {
        "id": "RNdUqXJyZRpi"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fa422c62"
      },
      "source": [
        "class ProjectionSoftmaxRegressor(nn.Module):\n",
        "    def __init__(self, num_features):\n",
        "        super(ProjectionSoftmaxRegressor, self).__init__()\n",
        "        # Initialize a learnable parameter W of shape (num_features, num_features)\n",
        "        self.A = nn.Parameter(torch.randn(num_features, num_features))\n",
        "\n",
        "    def forward(self, X_query, X_train_full, y_train_full):\n",
        "        # X_query: (batch_size, num_features)\n",
        "        # X_train_full.T: (num_features, num_training_samples)\n",
        "\n",
        "        # W: (num_features, num_features)\n",
        "        W = self.A.T @ self.A\n",
        "        # This ensures that W is symmetric and positive semi-definite,\n",
        "        # so we are learning a proper metric.\n",
        "\n",
        "        # Calculate scores: X_query @ W @ X_train_full.T\n",
        "        # scores: (batch_size, num_training_samples)\n",
        "        scores = X_query @ W @ X_train_full.T\n",
        "\n",
        "        # Apply softmax along the dimension corresponding to training samples\n",
        "        weights = torch.softmax(scores, dim=-1)\n",
        "\n",
        "        # Compute predictions as a weighted sum of y_train_full\n",
        "        # weights: (batch_size, num_training_samples)\n",
        "        # y_train_full: (num_training_samples, 1)\n",
        "        # predictions: (batch_size, 1)\n",
        "        predictions = weights @ y_train_full\n",
        "        return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_features = X_train_new.shape[1]\n",
        "model = ProjectionSoftmaxRegressor(num_features)\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "yefmFgpHfKMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a57a83d"
      },
      "source": [
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
        "\n",
        "# Early stopping parameters\n",
        "patience = 10\n",
        "min_val_loss = float('inf')\n",
        "trigger_times = 0\n",
        "\n",
        "# Set a reasonably high number of epochs, early stopping will prevent overfitting\n",
        "epochs = 20000\n",
        "\n",
        "# Store training and validation losses for plotting/analysis\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "print(\"Starting model training...\")\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Set model to training mode: this changes some behaviours\n",
        "    model.train()\n",
        "    current_train_loss = 0.0\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        # 4. Forward pass, loss calculation, backward pass, and optimizer step\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(batch_X, X_train_new, y_train_new) # X_train_new and y_train_new are full training data\n",
        "        loss = criterion(predictions, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        current_train_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = current_train_loss / len(train_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "\n",
        "    # 5. Evaluate the model on the val_loader\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    current_val_loss = 0.0\n",
        "    with torch.no_grad(): # Disable gradient calculation for validation\n",
        "        for batch_X_val, batch_y_val in val_loader:\n",
        "            predictions_val = model(batch_X_val, X_train_new, y_train_new)\n",
        "            val_loss = criterion(predictions_val, batch_y_val)\n",
        "            current_val_loss += val_loss.item()\n",
        "\n",
        "    avg_val_loss = current_val_loss / len(val_loader)\n",
        "    val_losses.append(avg_val_loss)\n",
        "\n",
        "    if (epoch + 1) % 250 == 0:\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
        "\n",
        "    # 6. Implement early stopping\n",
        "    if avg_val_loss < min_val_loss:\n",
        "        min_val_loss = avg_val_loss\n",
        "        trigger_times = 0\n",
        "        # Save the best model weights\n",
        "        torch.save(model.state_dict(), 'best_pretrained_softmax_regressor_model.pth')\n",
        "    else:\n",
        "        trigger_times += 1\n",
        "        if trigger_times >= patience:\n",
        "            print(f'Early stopping triggered after {epoch+1} epochs due to no improvement in validation loss.')\n",
        "            break\n",
        "\n",
        "print(\"Model training complete. Best model saved.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The story so far"
      ],
      "metadata": {
        "id": "qxXq5O0sfHnf"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "de2b6d01"
      },
      "source": [
        "model.load_state_dict(torch.load('best_pretrained_softmax_regressor_model.pth'))\n",
        "model.eval()\n",
        "\n",
        "all_predictions = []\n",
        "all_targets = []\n",
        "\n",
        "print(\"Evaluating model on the test set...\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_X_test, batch_y_test in test_loader:\n",
        "        predictions_test = model(batch_X_test, X_train_tensor, y_train_tensor)\n",
        "        all_predictions.append(predictions_test.cpu().numpy())\n",
        "        all_targets.append(batch_y_test.cpu().numpy())\n",
        "\n",
        "test_predictions = np.vstack(all_predictions)\n",
        "test_targets = np.vstack(all_targets)\n",
        "\n",
        "mse = mean_squared_error(test_targets, test_predictions)\n",
        "r2 = r2_score(test_targets, test_predictions)\n",
        "\n",
        "print(\"\\n--- Comparison with previous models ---\")\n",
        "print(f\"OLS Regressor - MSE: {mean_squared_error(y_test, ols_y_pred):.2f}, R2: {r2_score(y_test, ols_y_pred):.2f}\")\n",
        "print(f\"1-NN Regressor - MSE: {mean_squared_error(y_test, ols_1nn_y_pred):.2f}, R2: {r2_score(y_test, ols_1nn_y_pred):.2f}\")\n",
        "print(f\"{best_k}-NN Regressor - MSE: {best_knn_mse:.2f}, R2: {best_knn_r2:.2f}\")\n",
        "print(f\"Vanilla Dot Product Softmax Regressor - MSE: {mean_squared_error(y_test, dot_softmax_y_pred):.2f}, R2: {r2_score(y_test, dot_softmax_y_pred):.2f}\")\n",
        "print(f\"PyTorch ProjectionSoftmaxRegressor (Trained) - MSE: {mse:.2f}, R2: {r2:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (Almost) standard attention"
      ],
      "metadata": {
        "id": "cRO9-rQyfiuf"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3b640a5e"
      },
      "source": [
        "class AttentionRegressor(nn.Module):\n",
        "    def __init__(self, num_features, projection_dim=64):\n",
        "        super(AttentionRegressor, self).__init__()\n",
        "        self.projection_dim = projection_dim\n",
        "\n",
        "        # Initialize two learnable parameter matrices for query and train projections\n",
        "        self.W_query_proj = nn.Parameter(torch.randn(num_features, projection_dim))\n",
        "        self.W_train_proj = nn.Parameter(torch.randn(num_features, projection_dim))\n",
        "\n",
        "    def forward(self, X_query, X_train_full, y_train_full):\n",
        "        # Project X_query and X_train_full using their respective projection matrices\n",
        "        # X_query: (batch_size, num_features) -> X_query_projected: (batch_size, projection_dim)\n",
        "        # X_train_full: (num_training_samples, num_features) -> X_train_projected: (num_training_samples, projection_dim)\n",
        "        X_query_projected = X_query @ self.W_query_proj\n",
        "        X_train_projected = X_train_full @ self.W_train_proj\n",
        "\n",
        "        # Let's use more standard names and wrap things up.\n",
        "        Q = None\n",
        "        K = None\n",
        "        V = None\n",
        "        scores = None\n",
        "        weights = None\n",
        "        predictions = None\n",
        "\n",
        "        return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f57e02c3"
      },
      "source": [
        "# 1. Model instantiation\n",
        "num_features = X_train_new.shape[1]\n",
        "# We project inputs to a higher-dimensional space because relations between\n",
        "# data-points may be more transparent there\n",
        "projection_dim = 32\n",
        "model_projected = AttentionRegressor(\n",
        "    num_features,\n",
        "    projection_dim=projection_dim)\n",
        "model_projected.to(device)\n",
        "\n",
        "# 2. Definition of a loss function and an optimizer\n",
        "criterion_projected = nn.MSELoss()\n",
        "optimizer_projected = optim.AdamW(model_projected.parameters(), lr=0.001)\n",
        "\n",
        "# Early stopping parameters\n",
        "patience = 10  # Same patience as before\n",
        "min_val_loss_projected = float('inf')\n",
        "trigger_times_projected = 0\n",
        "\n",
        "epochs = 20000\n",
        "\n",
        "# Store training and validation losses for plotting/analysis\n",
        "train_losses_projected = []\n",
        "val_losses_projected = []\n",
        "\n",
        "print(\"Starting AttentionRegressor model training...\")\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model_projected.train() # Set model to training mode\n",
        "    current_train_loss_projected = 0.0\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        # 4. Forward pass, loss calculation, backward pass, and optimizer step\n",
        "        optimizer_projected.zero_grad()\n",
        "        # X_train_tensor and y_train_tensor are the full original training data, required by the model\n",
        "        predictions = model_projected(batch_X, X_train_tensor, y_train_tensor)\n",
        "        loss = criterion_projected(predictions, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer_projected.step()\n",
        "        current_train_loss_projected += loss.item()\n",
        "\n",
        "    avg_train_loss_projected = current_train_loss_projected / len(train_loader)\n",
        "    train_losses_projected.append(avg_train_loss_projected)\n",
        "\n",
        "    # 5. Evaluate the model on the val_loader\n",
        "    model_projected.eval() # Set model to evaluation mode\n",
        "    current_val_loss_projected = 0.0\n",
        "    with torch.no_grad(): # Disable gradient calculation for validation\n",
        "        for batch_X_val, batch_y_val in val_loader:\n",
        "            predictions_val = model_projected(batch_X_val, X_train_tensor, y_train_tensor)\n",
        "            val_loss = criterion_projected(predictions_val, batch_y_val)\n",
        "            current_val_loss_projected += val_loss.item()\n",
        "\n",
        "    avg_val_loss_projected = current_val_loss_projected / len(val_loader)\n",
        "    val_losses_projected.append(avg_val_loss_projected)\n",
        "\n",
        "    if (epoch + 1) % 250 == 0:\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss_projected:.4f}, Val Loss: {avg_val_loss_projected:.4f}')\n",
        "\n",
        "    # 6. Implement early stopping\n",
        "    if avg_val_loss_projected < min_val_loss_projected:\n",
        "        min_val_loss_projected = avg_val_loss_projected\n",
        "        trigger_times_projected = 0\n",
        "        # Save the best model weights\n",
        "        torch.save(model_projected.state_dict(), 'best_projected_softmax_regressor_model.pth')\n",
        "    else:\n",
        "        trigger_times_projected += 1\n",
        "        if trigger_times_projected >= patience:\n",
        "            print(f'Early stopping triggered after {epoch+1} epochs due to no improvement in validation loss.')\n",
        "            break\n",
        "\n",
        "print(\"AttentionRegressor model training complete. Best model saved.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "796cc3b7"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "model_projected.load_state_dict(torch.load('best_projected_softmax_regressor_model.pth'))\n",
        "model_projected.eval()\n",
        "\n",
        "all_predictions_projected = []\n",
        "all_targets_projected = []\n",
        "\n",
        "print(\"Evaluating AttentionRegressor on the test set...\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_X_test, batch_y_test in test_loader:\n",
        "        predictions_test_projected = model_projected(batch_X_test, X_train_tensor, y_train_tensor)\n",
        "        all_predictions_projected.append(predictions_test_projected.cpu().numpy())\n",
        "        all_targets_projected.append(batch_y_test.cpu().numpy())\n",
        "\n",
        "test_predictions_projected = np.vstack(all_predictions_projected)\n",
        "test_targets_projected = np.vstack(all_targets_projected)\n",
        "mse_projected = mean_squared_error(test_targets_projected, test_predictions_projected)\n",
        "r2_projected = r2_score(test_targets_projected, test_predictions_projected)\n",
        "\n",
        "print(f\"Mean Squared Error (PyTorch AttentionRegressor on Test Set): {mse_projected:.2f}\")\n",
        "print(f\"Coefficient of Determination (PyTorch AttentionRegressor on Test Set): {r2_projected:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why ‘almost’?\n",
        "\n",
        "In this exercise, we interpolated between different values of **y** based on pairwise distances between corresponding rows of X.\n",
        "\n",
        "In real attention, we interpolate the rows of X themselves. They correspond to input tokens and are ordered. Our goal is to find the most useful/informative linear combination of their values in order to predict something -- usually the next token in the sequence."
      ],
      "metadata": {
        "id": "YzCl6XufyCZP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HDPLJyAE40Jg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}