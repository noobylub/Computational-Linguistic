{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Week 7 Seminar: Logistic Regression with text"
      ],
      "metadata": {
        "id": "oHDBNKrzj5By"
      },
      "id": "oHDBNKrzj5By"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Matric-Vector multiplication\n",
        "\n",
        "In our model building so far we have been generating our predicted values by taking the dot product of vectors of weights and vectors of features (and adding the bias). However we want to move on to using more features than just 2. When we have lots of features, it is more efficient to handle the sets of j features for k items as a k x j matrix rather than as j vectors of length k. To combine our weights and our features we then need to take the dot product of each row of our feature matrix with our vector of weights. As you saw in this week's lecture we can take the dot product between a matrix with j columns and a row vector of length j. This can be done in numpy as follows:"
      ],
      "metadata": {
        "id": "Mbngjp6j8WJF"
      },
      "id": "Mbngjp6j8WJF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Matrix**: The matrix typically represents the weights connecting one layer of neurons to the next. Each row could correspond to a neuron in the next layer, and each column to a neuron in the previous layer. The values within the matrix are the weights that determine the strength of the connections.\n",
        "<br/>\n",
        "**Vector**: The vector can represent several things:\n",
        "Input features: For the first layer, it's a vector of the input features for a single data sample.\n",
        "Activations: For subsequent layers, it can be a vector of the activations (outputs) of the neurons in a layer.\n",
        "Bias: A bias term is often added as a vector to the result of the matrix multiplication."
      ],
      "metadata": {
        "id": "15GE_zAlIOyR"
      },
      "id": "15GE_zAlIOyR"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "vector = np.random.rand(3,1)\n",
        "matrix = np.random.rand(10,3)\n",
        "print(vector)\n",
        "print(matrix)\n",
        "# Matrix\n",
        "# You can imagine each row is one example of features\n",
        "# The columns would correspond to the different features of that example\n",
        "# Vector\n",
        "# Each column of the vector would then be the weights, or individual neurons\n"
      ],
      "metadata": {
        "id": "o6Ku_XzQkrDq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "435b20a2-ce44-4f3b-821a-0544a0934f49"
      },
      "id": "o6Ku_XzQkrDq",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.56874063]\n",
            " [0.24790583]\n",
            " [0.76767684]]\n",
            "[[0.89295395 0.23410506 0.71046302]\n",
            " [0.49964939 0.40835768 0.9337917 ]\n",
            " [0.48406699 0.75008352 0.59606309]\n",
            " [0.73514534 0.71121232 0.03776061]\n",
            " [0.57776487 0.20820304 0.92822414]\n",
            " [0.11096045 0.68995504 0.84446738]\n",
            " [0.16114388 0.27418738 0.73725047]\n",
            " [0.03610978 0.61664659 0.73118895]\n",
            " [0.70759209 0.15772089 0.62045276]\n",
            " [0.35537428 0.2047933  0.15219186]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "matrix.dot(vector) # or matrix@vector"
      ],
      "metadata": {
        "id": "jHqqKt2LoqGT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e343d19-e5dd-4253-96c0-2ef3561c9147"
      },
      "id": "jHqqKt2LoqGT",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.1113012 ],\n",
              "       [1.10225542],\n",
              "       [0.91884247],\n",
              "       [0.62340865],\n",
              "       [1.09278927],\n",
              "       [0.88242964],\n",
              "       [0.72559183],\n",
              "       [0.7347242 ],\n",
              "       [0.91784351],\n",
              "       [0.36971941]])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember that in matrix multiplication order is important. The matrix here cannot be left multiplied by the vector because its number of rows does not equal the number of elements in the vector. And so the following doesn't work:"
      ],
      "metadata": {
        "id": "8XEQjqjfymfR"
      },
      "id": "8XEQjqjfymfR"
    },
    {
      "cell_type": "code",
      "source": [
        "vector.dot(matrix)"
      ],
      "metadata": {
        "id": "qowx2--kvd9q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "e824a5ce-213c-4d93-9f93-43fd38c1d0d4"
      },
      "id": "qowx2--kvd9q",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "shapes (3,1) and (10,3) not aligned: 1 (dim 1) != 10 (dim 0)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2405934619.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m: shapes (3,1) and (10,3) not aligned: 1 (dim 1) != 10 (dim 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic regression with text: sentiment analysis\n",
        "\n",
        "The dataset we are going to use here is 10000 reviews on Yelp classified as negative (1 or 2 star) or positive (3 or 4 star). We are going to train a classifier using this a part of this data and test its performance on another part.\n",
        "\n",
        "First we download the dataset:"
      ],
      "metadata": {
        "id": "OC5U-WIK5AzZ"
      },
      "id": "OC5U-WIK5AzZ"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1d0fbf90",
      "metadata": {
        "id": "1d0fbf90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29f0596a-e11a-4822-d732-372eecdce11c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-10 14:20:23--  https://raw.githubusercontent.com/cbannard/lela60331_24-25/refs/heads/main/data/yelp_reviews.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7005742 (6.7M) [text/plain]\n",
            "Saving to: ‘yelp_reviews.txt’\n",
            "\n",
            "yelp_reviews.txt    100%[===================>]   6.68M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-11-10 14:20:23 (119 MB/s) - ‘yelp_reviews.txt’ saved [7005742/7005742]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/cbannard/lela60331_24-25/refs/heads/main/data/yelp_reviews.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset is in a tab-delimited file, so that the first element on each row is a review and then there is a tab and then the sentiment rating.\n",
        "\n",
        "We read it into to lists (one for reviews; one for labels) as follows:"
      ],
      "metadata": {
        "id": "XYB44ZXbRG4G"
      },
      "id": "XYB44ZXbRG4G"
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n4 yelp_reviews.txt"
      ],
      "metadata": {
        "id": "KSfGyP-Gnl5n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d8feeb2-e4a3-45ff-8831-195b15895e8a"
      },
      "id": "KSfGyP-Gnl5n",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "review\tlabel\n",
            "all i can say is that a i had no other option then to go with time warner cable for my internet service n nand b . . . . n nif you don t have a local number it will be an absolute nightmare to pay your bill ! you can go through the ring of automated messages only to get to a live person after min then be told they will charge you so you must go through the automated service . . . . repeat . i cringe when it comes time to pay my bill each month . n nif you are lucky enough to avoid using them i suggest you do so ! \tnegative\n",
            "i went here once when my long time stylist moved further and further away and cut her hours to one day per week . it was nearby , they had stylists that use my color line goldwell , and the place looked nice enough . unfortunately , after a hour apppointment i had booked and only planned for an hour and a half , i walked away with the worst color job and cut i ve had in a long time . even though i gave the stylist my formula . . and he said he thought my color looked perfect . . . he tried to upsell me on a more complicated and surprise , much more expensive process and service . learning experience for me , i guess . i ll happily drive minutes on a saturday for my girl angie who is now at unity salon near scottsdale and shea . \tnegative\n",
            "i don t know why i stopped here for lunch this afternoon . but i do know that i will never eat at a whataburger ever again ! n nthis place has some of the most off the wall people working the cash register ! this chick took my order then dosed off into a trance like stare at my yoohoo area . . . and no i didn t have anything going on down there or nothing on my pants if that s what your thinking . seriously though no joke i couldn t believe what this chick was doing in front of the entire lunch hour rush that was storming in and out . . . n ni looked at her then politely said umm hello ? miss , what are you looking at ? she came out of her spell and smiled at me with a kinda creepy half smile . . . then as i was filling up my drink i caught her doing it again ! n nno i m not gay and yes i do have standards . this chick was missing half her teeth and was about as old as bob barker . n nyeah i literally couldn t even eat cause this chick made me loose my appetite . and the fact that my burger and fries tasted like it was soaked in a bowl of grease then sat under a heat lamp for about minutes . n nthey should shut this place down or turn it into a in n out burger . never again ! \tnegative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create lists\n",
        "reviews=[]\n",
        "labels=[]\n",
        "\n",
        "with open(\"yelp_reviews.txt\") as f:\n",
        "   # iterate over the lines in the file\n",
        "   for line in f.readlines()[1:]:\n",
        "        # split the current line into a list of two element - the review and the label\n",
        "        fields = line.rstrip().split('\\t')\n",
        "        # put the current review in the reviews list\n",
        "        reviews.append(fields[0])\n",
        "        # put the current sentiment rating in the labels list\n",
        "        labels.append(fields[1])\n",
        "\n"
      ],
      "metadata": {
        "id": "KMbHKlsjz1wH"
      },
      "id": "KMbHKlsjz1wH",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to represent our data using one-hot encoding. We need to use the same vocabulary for our training and test data so we do this prior to splitting the data.\n",
        "\n",
        "In order to one-hot encode we need to create a list of the included vocabulary items. We will use the 5000 most frequent words (not an ideal solution but a convenient one). To get this list we extract all the words from all the reviews, count how often they occur, sort them and then take the most frequent 5000 words. To count the words we are going to use the Counter object from the collections module."
      ],
      "metadata": {
        "id": "0hqHqxKGSJwb"
      },
      "id": "0hqHqxKGSJwb"
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import re\n",
        "# Tokenise the text, turning a list of strings into a list of lists of tokens. We use very naive space-based tokenisation.\n",
        "tokenized_sents = [re.findall(\"[^ ]+\",txt) for txt in reviews]\n",
        "# Collapse all tokens into a single list\n",
        "tokens=[]\n",
        "for s in tokenized_sents:\n",
        "      tokens.extend(s)\n",
        "# Count the tokens in the tokens list. The returns a list of tuples of each token and count\n",
        "counts=Counter(tokens)\n",
        "# Sort the tuples. The reverse argument instructs to put most frequent first rather than last (which is the default)\n",
        "so=sorted(counts.items(), key=lambda item: item[1], reverse=True)\n",
        "# Extract the list of tokens, by transposing the list of lists so that there is a list of tokens a list of counts and then just selecting the former\n",
        "so=list(zip(*so))[0]\n",
        "# Select the firs 5000 words in the list\n",
        "type_list=so[0:5000]"
      ],
      "metadata": {
        "id": "rXVFg3H442fc"
      },
      "id": "rXVFg3H442fc",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type_list"
      ],
      "metadata": {
        "id": "wrZ2wG8ZKaXq"
      },
      "id": "wrZ2wG8ZKaXq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are now ready to one-hot encode our reviews. We have 10000 reviews and a selected vocabulary of 5000 words. We therefore want to end up with 10000 x 5000 matrix **M**, where each row $i$ is a review, each column $j$ is a unique word from the vocab, and each element $x_{i,j}$ is a one if the word j occurs in review i and a zero otherwise."
      ],
      "metadata": {
        "id": "E5SAuA6QaZXf"
      },
      "id": "E5SAuA6QaZXf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 1: Create M"
      ],
      "metadata": {
        "id": "AjmVhn1RoO0m"
      },
      "id": "AjmVhn1RoO0m"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a 10000 x 5000 matrix of zeros\n",
        "M = np.zeros((len(reviews), len(type_list)))\n",
        "#iterate over the reviews\n",
        "for i, rev in enumerate(reviews):\n",
        "  for(j,word) in enumerate(type_list):\n",
        "    if word in rev:\n",
        "      M[i,j]=1\n",
        "\n"
      ],
      "metadata": {
        "id": "5HgWE38SMIFk"
      },
      "id": "5HgWE38SMIFk",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "M"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C15U-weKLSVO",
        "outputId": "5a328182-7a27-44e4-f68c-68f9ffe7871c"
      },
      "id": "C15U-weKLSVO",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 1., 0., ..., 0., 0., 0.],\n",
              "       [1., 1., 1., ..., 0., 0., 0.],\n",
              "       [1., 1., 1., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [1., 0., 1., ..., 0., 0., 0.],\n",
              "       [1., 1., 0., ..., 0., 0., 0.],\n",
              "       [1., 1., 1., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are ready to split our data. We are going to use 20% of our data as test items, so we randomly select 8000 indices between 0 and 10000, which are the indices of our training items. The remaining 2000 indices are the indices of our test items.\n",
        "\n",
        "In a real development task we would want to split data into training, development and test. Here we just use training and test."
      ],
      "metadata": {
        "id": "ccEkUhsheHR6"
      },
      "id": "ccEkUhsheHR6"
    },
    {
      "cell_type": "code",
      "source": [
        "train_ints=np.random.choice(len(reviews),int(len(reviews)*0.8),replace=False)\n",
        "test_ints=list(set(range(0,len(reviews))) - set(train_ints))"
      ],
      "metadata": {
        "id": "JjTsjjDfCa5p"
      },
      "id": "JjTsjjDfCa5p",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We next use the indices to select the rows of our one-hot-encoded matrix M that correspond to our training items and our test items and put these into two separate matrices. We also select the corresponding labels."
      ],
      "metadata": {
        "id": "cDyfEsoRevxE"
      },
      "id": "cDyfEsoRevxE"
    },
    {
      "cell_type": "code",
      "source": [
        "M_train = M[train_ints,]\n",
        "M_test = M[test_ints,]\n",
        "labels_train = [labels[i] for i in train_ints]\n",
        "labels_test = [labels[i] for i in test_ints]"
      ],
      "metadata": {
        "id": "7GzcI2BeDWap"
      },
      "id": "7GzcI2BeDWap",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are ready to train our model using the training data."
      ],
      "metadata": {
        "id": "Keub330AfSJv"
      },
      "id": "Keub330AfSJv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 2 Complete the code below so that it learns a logistic regression model from the training data."
      ],
      "metadata": {
        "id": "mN_odk43faPo"
      },
      "id": "mN_odk43faPo"
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "num_features=5000\n",
        "y=[int(l == \"positive\") for l in labels_train]\n",
        "weights = np.random.rand(num_features)\n",
        "bias=np.random.rand(1)\n",
        "n_iters = 500\n",
        "lr=0.4\n",
        "logistic_loss=[]\n",
        "num_samples=len(y)\n",
        "for i in range(n_iters):\n",
        "  z=????\n",
        "  q =?????\n",
        "  eps=0.00001\n",
        "  loss = -sum((y*np.log2(q+eps)+(np.ones(len(y))-y)*np.log2(np.ones(len(y))-q+eps)))\n",
        "  logistic_loss.append(loss)\n",
        "  y_pred=[int(ql > 0.5) for ql in q]\n",
        "\n",
        "\n",
        "  dw = ???\n",
        "  db = ????\n",
        "  weights = ????\n",
        "  bias = ?????\n",
        "\n",
        "plt.plot(range(1,n_iters),logistic_loss[1:])\n",
        "plt.xlabel(\"number of epochs\")\n",
        "plt.ylabel(\"loss\")\n",
        "#loss = sum(-(np.ones(len(y))*np.log2(q)+(np.ones(len(y))-y)*np.log2(np.ones(len(y))-q)))"
      ],
      "metadata": {
        "id": "My1xq84sUqUY"
      },
      "id": "My1xq84sUqUY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have a fitting model, we can use it to predict labels for our test items. The test reviews are in the one-hot matrix M_test. The labels for the test reviews are in the list labels_test.\n",
        "\n",
        "Problem 3: Complete the code below so that it calculate the vector of predicted values y_test_pred for our test items."
      ],
      "metadata": {
        "id": "-l-FYqk3f58c"
      },
      "id": "-l-FYqk3f58c"
    },
    {
      "cell_type": "code",
      "source": [
        "  z= ????\n",
        "  q = ????\n",
        "  y_test_pred=[int(ql > 0.5) for ql in q]"
      ],
      "metadata": {
        "id": "Wcl2h0B3I4lk"
      },
      "id": "Wcl2h0B3I4lk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can calculate accuracy for the performance of our model on the test items as follows:"
      ],
      "metadata": {
        "id": "m77hqcRzgn8K"
      },
      "id": "m77hqcRzgn8K"
    },
    {
      "cell_type": "code",
      "source": [
        "y_test=[int(l == \"positive\") for l in labels_test]\n",
        "acc_test=[int(yp == y_test[s]) for s,yp in enumerate(y_test_pred)]\n",
        "print(sum(acc_test)/len(acc_test))"
      ],
      "metadata": {
        "id": "ZOz9DW4LI97R"
      },
      "id": "ZOz9DW4LI97R",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember though that precision is not usually a good measure and so we calculate precision and recall.\n",
        "\n",
        "Problem 4 : Calculate precision and recall values for the performance of our model on the test data. I have given code for calculating the true positive rate. You will need to calculate the rest of the values from the confusion matrix and then use these numbers to calculate our evaluation metrics."
      ],
      "metadata": {
        "id": "zs9xzOnSgw3C"
      },
      "id": "zs9xzOnSgw3C"
    },
    {
      "cell_type": "code",
      "source": [
        "labels_test_pred=[\"positive\" if s == 1 else \"negative\" for s in y_test_pred]"
      ],
      "metadata": {
        "id": "hjd74SwOLUYg"
      },
      "id": "hjd74SwOLUYg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "true_positives=sum([int(yp == \"positive\" and labels_test[s] == \"positive\") for s,yp in enumerate(labels_test_pred)])"
      ],
      "metadata": {
        "id": "X1Nc267eNiAA"
      },
      "id": "X1Nc267eNiAA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 5: Calculate precision and recall values for the performance of the model on the training data. How do the numbers differ from those you found for the test set? Why do you think this is?"
      ],
      "metadata": {
        "id": "UnGlulqXhPd-"
      },
      "id": "UnGlulqXhPd-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inspecting weights\n",
        "Weights in a one-layer network with one-hot encoded inputs are simple to interpret - words with high weights are those whose presence makes a positive outcome most likely and those with low weights are those whose presence makes a negative outcome most likely.\n",
        "\n",
        "Our weights are in a numpy array, with each weight corresponding the word at the same position (index) in our type_list.\n",
        "\n",
        " We can obtain a list of the indices in order to the weight at each position using the numpy function argsort. This sorts from low to high so we take the top 10 to find the indices of the ten words most strongly associated with a negative outcome. Or we can reverse it to find the indices of the ten words most strongly associated with a positive outcome.\n",
        "\n",
        " We can ten just look at the words that occur at these indices:"
      ],
      "metadata": {
        "id": "Wz84asazpYcW"
      },
      "id": "Wz84asazpYcW"
    },
    {
      "cell_type": "code",
      "source": [
        "[type_list[x] for x in np.argsort(weights)[0:20]]"
      ],
      "metadata": {
        "id": "_n7q8sz2yj5s"
      },
      "execution_count": null,
      "outputs": [],
      "id": "_n7q8sz2yj5s"
    },
    {
      "cell_type": "code",
      "source": [
        "[type_list[x] for x in np.argsort(weights)[::-1][0:20]]"
      ],
      "metadata": {
        "id": "TjRGV1itofiD"
      },
      "execution_count": null,
      "outputs": [],
      "id": "TjRGV1itofiD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 6a: Inspect the list of tokens with highest and lowest weights, going beyond the top and bottom 10. Do you spot any that indicate overfitting?\n",
        "\n",
        "Problem 6b: Look back up to the first code block above where we create the one-hot encoded data. Can you change the way the text is tokenised in order to avoid the problematic tokens you noted in 6a? Retrain the model. How do your changes affect performance / the token lists?\n"
      ],
      "metadata": {
        "id": "9HOxf9Qn1lP8"
      },
      "id": "9HOxf9Qn1lP8"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}